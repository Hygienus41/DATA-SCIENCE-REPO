{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca51b8ce-0e0f-4695-b5db-4a6901eb5440",
   "metadata": {},
   "source": [
    "# Preprocessing for Machine Learning in Python\n",
    "\n",
    "## Introduction to Data Preprocessing\n",
    "\n",
    "\n",
    "In this chapter you'll learn exactly what it means to preprocess data. You'll take the first steps in any preprocessing journey, including exploring data types and dealing with missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b99aca-4873-456b-bbd1-4f59eb8fb03c",
   "metadata": {},
   "source": [
    "# Exploring missing data\n",
    "\n",
    "You've been given a dataset comprised of volunteer information from New York City, stored in the volunteer DataFrame. Explore the dataset using the plethora of methods and attributes pandas has to offer to answer the following question.\n",
    "\n",
    "How many missing values are in the locality column?\n",
    "\n",
    "volunteer[\"locality\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12172b6f-705a-402d-9934-71d1814fbcb8",
   "metadata": {},
   "source": [
    "# Dropping missing data\n",
    "\n",
    "Now that you've explored the volunteer dataset and understand its structure and contents, it's time to begin dropping missing values.\n",
    "\n",
    "In this exercise, you'll drop both columns and rows to create a subset of the volunteer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d790f59-1c4a-4f49-be92-e85e9137a3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the Latitude and Longitude columns from volunteer\n",
    "volunteer_cols = volunteer.drop([\"Latitude\",\"Longitude\"],axis=1)\n",
    "\n",
    "# Drop rows with missing category_desc values from volunteer_cols(Subset volunteer_cols by dropping rows containing missing values in the category_desc, and store in a new variable called volunteer_subset.)\n",
    "volunteer_subset = volunteer_cols.dropna(subset=[\"category_desc\"])\n",
    "\n",
    "# Print out the shape of the volunteer_subset(Take a look at the .shape attribute of volunteer_subset, to verify it worked correctly.)\n",
    "print(volunteer_subset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f824f970-0d16-4006-a363-e1e2206d97e5",
   "metadata": {},
   "source": [
    "# working with Data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e083ce-e0ef-4acd-bcef-5b8ad7496908",
   "metadata": {},
   "source": [
    "# Exploring data types\n",
    "Taking another look at the dataset comprised of volunteer information from New York City, you want to know what types you'll be working with as you start to do more preprocessing.\n",
    "\n",
    "Which data types are present in the volunteer dataset?\n",
    "\n",
    "volunteer.info()\n",
    "\n",
    "Floats, integers, and objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454b6f79-2129-4d01-b687-08bf3c6d3dbf",
   "metadata": {},
   "source": [
    "# Converting a column type\n",
    "\n",
    "If you take a look at the volunteer dataset types, you'll see that the column hits is type object. But, if you actually look at the column, you'll see that it consists of integers. Let's convert that column to type int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73fe5b3-7697-4bdc-8108-1d63e6363e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the head of the hits column\n",
    "print(volunteer[\"hits\"].head())\n",
    "\n",
    "# Convert the hits column to type int\n",
    "volunteer[\"hits\"] = volunteer[\"hits\"].astype(int)\n",
    "\n",
    "# Take a look at the .dtypes of the dataset again, and notice that the column type has changed.\n",
    "print(volunteer.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0321afb3-bdb0-4b2c-b43d-d8ee9cffe241",
   "metadata": {},
   "source": [
    "# Training and test set\n",
    "\n",
    "Class imbalance\n",
    "In the volunteer dataset, you're thinking about trying to predict the category_desc variable using the other features in the dataset. First, though, you need to know what the class distribution (and imbalance) is for that label.\n",
    "\n",
    "Which descriptions occur less than 50 times in the volunteer dataset?\n",
    "\n",
    "\n",
    "Possible answers\n",
    "\n",
    "\n",
    "-Emergency Preparedness, -Health\n",
    "\n",
    "-Environment, -Environment and Emergency Preparedness\n",
    "\n",
    "All of the above\n",
    "\n",
    "### code solution\n",
    "\n",
    "category_counts = volunteer['category_desc'].value_counts()\n",
    "\n",
    "rare_categories = category_counts[category_counts < 50]\n",
    "\n",
    "rare_categories\n",
    "\n",
    "Out[1]:\n",
    "\n",
    "category_desc\n",
    "\n",
    "Environment               32\n",
    "\n",
    "Emergency Preparedness    15\n",
    "\n",
    "Name: count, dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ef1a0c-3cbb-4aed-849c-68a2dc2ca312",
   "metadata": {},
   "source": [
    "# Stratified sampling\n",
    "\n",
    "You now know that the distribution of class labels in the category_desc column of the volunteer dataset is uneven. If you wanted to train a model to predict category_desc, you'll need to ensure that the model is trained on a sample of data that is representative of the entire dataset. Stratified sampling is a way to achieve this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83383569-6660-4d2a-805d-8eb4fb9d1635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with all columns except category_desc\n",
    "X = volunteer.drop(\"category_desc\", axis=1)\n",
    "\n",
    "# Create a category_desc labels dataset\n",
    "y = volunteer[[\"category_desc\"]]\n",
    "\n",
    "# Use stratified sampling to split up the dataset according to the y dataset(Split X and y into training and test sets, ensuring that the class distribution in the labels is the same in both sets)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Print the category_desc counts from y_train\n",
    "print(y_train[\"category_desc\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4dab0c-b3a5-4b23-8e37-76c78fc53274",
   "metadata": {},
   "source": [
    "# Standardizing Data\n",
    "\n",
    "This chapter is all about standardizing data. Often a model will make some assumptions about the distribution or scale of your features. Standardization is a way to make your data fit these assumptions and improve the algorithm's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eb05c7-e004-4b0c-a944-745040ad998a",
   "metadata": {},
   "source": [
    "# When to standardize\n",
    "reasons to standardize?\n",
    "\n",
    "\n",
    "A column you want to use for modeling has extremely high variance.\n",
    "\n",
    "You have a dataset with several continuous columns on different scales, and you'd like to use a linear model to train the data.\n",
    "\n",
    "The models you're working with use some sort of distance metric in a linear space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3565a4c1-dd30-4ec7-b0e4-6d928cbbfc80",
   "metadata": {},
   "source": [
    "# Modeling without normalizing\n",
    "\n",
    "Let's take a look at what might happen to your model's accuracy if you try to model data without doing some sort of standardization first.\n",
    "\n",
    "Here we have a subset of the wine dataset. One of the columns, Proline, has an extremely high variance compared to the other columns. This is an example of where a technique like log normalization would come in handy, which you'll learn about in the next section.\n",
    "\n",
    "The scikit-learn model training process should be familiar to you at this point, so we won't go too in-depth with it. You already have a k-nearest neighbors model available (knn) as well as the X and y sets you need to fit and score on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c356fb-2a30-4e7e-93f5-8b9541797dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up the X and y sets into training and test sets, ensuring that class labels are equally distributed in both sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Fit the knn model to the training features and labels.\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Print the test set accuracy of the knn model using the .score() method.\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90a5cc1-8b4f-4781-aa3e-333474d1db94",
   "metadata": {},
   "source": [
    "# Log Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510c2c9b-1432-40a0-b48b-ba0e9dfe9f50",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Checking the variance\n",
    "\n",
    "Check the variance of the columns in the wine dataset. Out of the four columns listed, which column is the most appropriate candidate for normalization?\n",
    "\n",
    "wine.var()\n",
    "\n",
    "Out[1]:\n",
    "\n",
    "Type                                0.601\n",
    "\n",
    "Alcohol                             0.659\n",
    "\n",
    "Malic acid                          1.248\n",
    "\n",
    "Ash                                 0.075\n",
    "\n",
    "Alcalinity of ash                  11.153\n",
    "\n",
    "Magnesium                         203.989\n",
    "\n",
    "Total phenols                       0.392\n",
    "\n",
    "Color intensity                     5.\n",
    "\n",
    "Proline                         99166.717\n",
    "\n",
    "dtype: float64\n",
    "\n",
    "Answer is proline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473c96b9-c185-4a79-93ad-24abc597bcae",
   "metadata": {},
   "source": [
    "# Log normalization in Python\n",
    "\n",
    "Now that we know that the Proline column in our wine dataset has a large amount of variance, let's log normalize it.\n",
    "\n",
    "numpy has been imported as np."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63842bb-e7fb-4392-9fce-6adc9e4c2f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the variance of the Proline column\n",
    "print(wine[\"Proline\"].var())\n",
    "\n",
    "# Use the np.log() function on the Proline column to create a new, log-normalized column named Proline_log\n",
    "wine[\"Proline_log\"] = np.log(wine[\"Proline\"])\n",
    "\n",
    "# Check the variance of the normalized Proline column\n",
    "print(wine[\"Proline_log\"].var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f55ee8e-d45a-491c-8d8e-559f305e853d",
   "metadata": {},
   "source": [
    "# Scaling data - investigating columns\n",
    "\n",
    "You want to use the Ash, Alcalinity of ash, and Magnesium columns in the wine dataset to train a linear model, but it's possible that these columns are all measured in different ways, which would bias a linear model.\n",
    "\n",
    "Which of the following statements about these columns is true?\n",
    "\n",
    "answer\n",
    "\n",
    "The max of Ash is 3.23, the max of Alcalinity of ash is 30, and the max of Magnesium is 162."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f133c6f0-310a-4846-8e93-7ef1958b9e91",
   "metadata": {},
   "source": [
    "# Scaling data - standardizing columns\n",
    "\n",
    "Since we know that the Ash, Alcalinity of ash, and Magnesium columns in the wine dataset are all on different scales, let's standardize them in a way that allows for use in a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc67bd48-cc6e-420f-a036-4c4627b466e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate a StandardScaler() and store it in the variable, scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create a subset of the wine DataFrame containing the Ash, Alcalinity of ash, and Magnesium columns, assign it to wine_subset\n",
    "wine_subset = wine[[\"Ash\", \"Alcalinity of ash\", \"Magnesium\"]]\n",
    "\n",
    "# Apply the scaler to wine_subset(Fit and transform the standard scaler to wine_subset.)\n",
    "wine_subset_scaled = scaler.fit_transform(wine_subset)\n",
    "\n",
    "# In scikit-learn, running .fit_transform() during preprocessing will both fit the method to the data as well as transform the data in a single step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967b937e-901d-4104-9495-0f3dcb2b8773",
   "metadata": {},
   "source": [
    "# Standardized data and modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa74aea2-cd14-4d9f-a09b-1a91c80b0f92",
   "metadata": {},
   "source": [
    "# KNN on non-scaled data\n",
    "\n",
    "Before adding standardization to your scikit-learn workflow, you'll first take a look at the accuracy of a K-nearest neighbors model on the wine dataset without standardizing the data.\n",
    "\n",
    "The knn model as well as the X and y data and labels sets have been created already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a148ba15-7562-4f97-9384-1e451675dbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset and labels into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "# Score the model on the test data(Print out the test set accuracy of your trained knn model)\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9b8aac-cafb-43f3-9e3f-947ab7687ae2",
   "metadata": {},
   "source": [
    "Well done! This accuracy definitely isn't poor, but let's see if we can improve it by standardizing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfdebc0-4dc8-4783-b759-d04079d5be95",
   "metadata": {},
   "source": [
    "# KNN on scaled data\n",
    "\n",
    "The accuracy score on the unscaled wine dataset was decent, but let's see what you can achieve by using standardization. Once again, the knn model as well as the X and y data and labels set have already been created for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c77bf-069c-4c10-a17b-6618043724d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Create the StandardScaler() method, stored in a variable named scaler.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the training and test features, being careful not to introduce data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the scaled training data\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model's performance by computing the test set accuracy\n",
    "print(knn.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe3655b-3aa3-4464-9328-e38514000b68",
   "metadata": {},
   "source": [
    "That's quite the improvement, and definitely made scaling the data worthwhile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5c25e3-efde-45ee-a5a1-34d6f87517f2",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3f3415-57b9-4692-8759-74a747f72768",
   "metadata": {},
   "source": [
    "# Feature engineering knowledge test\n",
    "Now that you've learned about feature engineering, which of the following examples are good candidates for creating new features?\n",
    "\n",
    "A column of timestamps\n",
    "\n",
    "A column of newspaper headlines\n",
    "\n",
    "Correct! Timestamps can be broken into days or months, and headlines can be used for natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b616803-c832-48d6-8f99-aba42398541a",
   "metadata": {},
   "source": [
    "# Encoding categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9336ec0a-3a66-4f17-bdb9-0051c1eb16af",
   "metadata": {},
   "source": [
    "# Encoding categorical variables - binary\n",
    "\n",
    "Take a look at the hiking dataset. There are several columns here that need encoding before they can be modeled, one of which is the Accessible column. Accessible is a binary feature, so it has two values, Y or N, which need to be encoded into 1's and 0's. Use scikit-learn's LabelEncoder method to perform this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af5c9c4-aae9-4fc7-8734-bc1ba84fb19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LabelEncoder object\n",
    "enc = LabelEncoder()\n",
    "\n",
    "# Apply the encoding to the \"Accessible\" column(Using the encoder's .fit_transform() method, encode the hiking dataset's \"Accessible\" column. Call the new column Accessible_enc)\n",
    "hiking[\"Accessible_enc\" ]= enc.fit_transform(hiking[\"Accessible\"])\n",
    "\n",
    "# Compare the two columns side-by-side to see the encoding.\n",
    "print(hiking[[\"Accessible\", \"Accessible_enc\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d350277-f62d-46bc-aaad-81d16c8c4aea",
   "metadata": {},
   "source": [
    ".fit_transform() is a good way to both fit an encoding and transform the data in a single step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32a0bba-384d-4971-90ea-672af48f6ce9",
   "metadata": {},
   "source": [
    "# Encoding categorical variables - one-hot\n",
    "\n",
    "One of the columns in the volunteer dataset, category_desc, gives category descriptions for the volunteer opportunities listed. Because it is a categorical variable with more than two categories, we need to use one-hot encoding to transform this column numerically. Use pandas' pd.get_dummies() function to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2241f1-3af1-4c84-9528-81c943ebee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call get_dummies() on the volunteer[\"category_desc\"] column to create the encoded columns and assign it to category_enc.\n",
    "category_enc = pd.get_dummies(volunteer[\"category_desc\"])\n",
    "\n",
    "# Print out the .head() of the category_enc variable to take a look at the encoded columns.\n",
    "print(category_enc.head())\n",
    "\n",
    "# get_dummies() is a simple and quick way to encode categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bbdbb0-34d6-4e44-a99b-d37ed414fb06",
   "metadata": {},
   "source": [
    "# Engineering numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1137f90f-a5c5-4c6e-886a-308ddab116e7",
   "metadata": {},
   "source": [
    "# Aggregating numerical features\n",
    "\n",
    "A good use case for taking an aggregate statistic to create a new feature is when you have many features with similar, related values. Here, you have a DataFrame of running times named running_times_5k. For each name in the dataset, take the mean of their 5 run times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aae1366-d508-4fba-bf3c-ebc32b0893c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the .loc[] method to select all rows and columns to find the .mean() of the each columns.\n",
    "running_times_5k[\"mean\"] = running_times_5k.loc[:, \"run1\":\"run5\"].mean(axis= 1)\n",
    "\n",
    "# Print the .head() of the DataFrame to see the mean column.\n",
    "print(running_times_5k.head())\n",
    "\n",
    "# .loc[] is especially helpful for operating across columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952765ca-0999-409c-ac00-069c177e1eb2",
   "metadata": {},
   "source": [
    "# Extracting datetime components\n",
    "\n",
    "There are several columns in the volunteer dataset comprised of datetimes. Let's take a look at the start_date_date column and extract just the month to use as a feature for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21282323-e1ba-474b-b03e-2f1576f721ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, Convert the start_date_date column into a pandas datetime column and store it in a new column called start_date_converted.\n",
    "volunteer[\"start_date_converted\"] = pd.to_datetime(volunteer[\"start_date_date\"])\n",
    "\n",
    "# Extract just the month from the converted column(Retrieve the month component of start_date_converted and store it in a new column called start_date_month)\n",
    "volunteer[\"start_date_month\"] = volunteer[\"start_date_converted\"].dt.month\n",
    "\n",
    "# Take a look at the converted and new month columns(Print the .head() of just the start_date_converted and start_date_month columns)\n",
    "print(volunteer[[\"start_date_converted\", \"start_date_month\"]].head())\n",
    "\n",
    "# You can also use attributes like .day to get the day and .year to get the year from datetime columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58a9b78-9f08-4866-afc5-5f18530cb874",
   "metadata": {},
   "source": [
    "# Engineering text features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2ce30b-3da6-460e-9319-7d006925b400",
   "metadata": {},
   "source": [
    "# Extracting string patterns\n",
    "\n",
    "The Length column in the hiking dataset is a column of strings, but contained in the column is the mileage for the hike. We're going to extract this mileage using regular expressions, and then use a lambda in pandas to apply the extraction to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9de952e-2b3e-4bcc-9145-1a8caf6a4335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a pattern to extract numbers and decimals\n",
    "def return_mileage(length):\n",
    "    \n",
    "    # Search the text in the length argument for numbers and decimals using an appropriate pattern.\n",
    "    mile = re.search(\"\\d+\\.\\d+\", length)\n",
    "    \n",
    "    # If a value is returned, use group(0) to return the found value(Extract the matched pattern and convert it to a float.)\n",
    "    if mile is not None:\n",
    "        return float(mile.group(0))\n",
    "        \n",
    "# Apply the return_mileage() function to each row in the hiking[\"Length\"] column.\n",
    "hiking[\"Length_num\"] = hiking[\"Length\"].apply(return_mileage)\n",
    "print(hiking[[\"Length\", \"Length_num\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c80e6c-abb3-41f2-a5b5-77a287f995e7",
   "metadata": {},
   "source": [
    "# Vectorizing text\n",
    "\n",
    "You'll now transform the volunteer dataset's title column into a text vector, which you'll use in a prediction task in the next exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8489f155-ea41-4b50-bc88-4322e9f77250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the volunteer[\"title\"] column in a variable named title_text\n",
    "title_text = volunteer[\"title\"]\n",
    "\n",
    "# Instantiate a TfidfVectorizer as tfidf_vec\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "# Transform the text in title_text into a tf-idf vector using tfidf_vec\n",
    "text_tfidf = tfidf_vec.fit_transform(title_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85697557-a985-42e6-85df-ab17add33bc9",
   "metadata": {},
   "source": [
    "scikit-learn provides several methods for text vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e72cb4-e1dd-48bb-8b8a-d13a6c3ebf95",
   "metadata": {},
   "source": [
    "# Text classification using tf/idf vectors\n",
    "\n",
    "Now that you've encoded the volunteer dataset's title column into tf/idf vectors, you'll use those vectors to predict the category_desc column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c260a4b2-a924-4177-b1d3-a86ea0ed96bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text_tfidf vector and y target variable into training and test sets, setting the stratify parameter equal to y, since the class distribution is uneven. Notice that we have to run the .toarray() method on the tf/idf vector, in order to get in it the proper format for scikit-learn.\n",
    "y = volunteer[\"category_desc\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y, random_state=42)\n",
    "\n",
    "# Fit the X_train and y_train data to the Naive Bayes model, nb.\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Print out the model's accuracy(ie test set accuracy)\n",
    "print(nb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df4fb08-a56f-4961-8948-89b36e8eb074",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "\n",
    "    0.5161290322580645\n",
    "\n",
    "Nice work! Notice that the model doesn't score very well. We'll work on selecting the best features for modeling in the next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e934c19-f828-4a75-8019-cb2703b87923",
   "metadata": {},
   "source": [
    "# Selecting Features for Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68c78df-7d51-471a-b24b-bb06dd275590",
   "metadata": {},
   "source": [
    "# When to use feature selection\n",
    "You've finished standardizing your data and creating new features. Which of the following scenarios is NOT a good candidate for feature selection?\n",
    "\n",
    "Select one answer\n",
    "\n",
    "Several columns of running times have been averaged into a new column\n",
    "\n",
    "A text field that hasn't been turned into a tf/idf vector yet (correct) The text field needs to be vectorized before removing it, otherwise we might lose important data.\n",
    "\n",
    "A column of text that has had a float extracted from it\n",
    "\n",
    "A categorical field that has been one-hot encoded\n",
    "\n",
    "There are columns related to whether something is a fruit or vegetable, the name of the fruit or vegetable, and the scientific name of the plant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f03be27-bff3-4a8b-ab42-8561560d4a35",
   "metadata": {},
   "source": [
    "# Removing redundant features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8df20a-2c87-40ea-a0ab-87b69ea56e89",
   "metadata": {},
   "source": [
    "# Selecting relevant features\n",
    "In this exercise, you'll identify the redundant columns in the volunteer dataset, and perform feature selection on the dataset to return a DataFrame of the relevant features.\n",
    "\n",
    "For example, if you explore the volunteer dataset in the console, you'll see three features which are related to location: locality, region, and postalcode. They contain related information, so it would make sense to keep only one of the features.\n",
    "\n",
    "Take some time to examine the features of volunteer in the console, and try to identify the redundant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ad64e-4d86-4bec-95c1-1c7ab228f3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of redundant column names to drop\n",
    "to_drop = [\"category_desc\", \"created_date\", \"locality\", \"region\", \"vol_requests\"]\n",
    "\n",
    "# Drop those columns from the dataset\n",
    "volunteer_subset = volunteer.drop(to_drop, axis=1)\n",
    "\n",
    "# Print out the head of volunteer_subset\n",
    "print(volunteer_subset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cc63c0-dc2e-4d0f-893e-23a8add7c159",
   "metadata": {},
   "source": [
    "# Checking for correlated features\n",
    "\n",
    "You'll now return to the wine dataset, which consists of continuous, numerical features. Run Pearson's correlation coefficient on the dataset to determine which columns are good candidates for eliminating. Then, remove those columns from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7519ea17-3888-40f5-8573-7112b9e233d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the Pearson correlation coefficients for each pair of features in the wine dataset.\n",
    "print(wine.corr())\n",
    "\n",
    "# Drop any columns from wine that have a correlation coefficient above 0.75 with at least two other columns.\n",
    "wine = wine.drop(\"Flavanoids\", axis=1)\n",
    "\n",
    "print(wine.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d657ae2-be1e-4957-b912-ab37960fe619",
   "metadata": {},
   "source": [
    "Dropping correlated features is often an iterative process, so you may need to try different combinations in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be88b9e-6476-4dd6-bd61-511dbf516116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17af6f58-f170-4e89-a5a6-3f515a510674",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fb7fd39-d8e3-479c-b453-a4cfb607987d",
   "metadata": {},
   "source": [
    "# Your recent learnings\n",
    "\n",
    "When you left 21 hours ago, you worked on Selecting Features for Modeling, chapter 4 of the course Preprocessing for Machine Learning in Python. Here is what you covered in your last lesson:\n",
    "\n",
    "You learned about selecting the most important features for your modeling tasks, focusing on removing redundant features from your dataset. Redundant features are those that are unnecessary for modeling because they either duplicate information present in other features or are too strongly correlated with other features. Key points covered include:\n",
    "\n",
    "The importance of dropping redundant features to avoid noise in your models. For instance, if two features are strongly correlated, keeping both can introduce bias.\n",
    "How to identify redundant features, such as through manual inspection for repeated information or using Pearson's correlation coefficient for numerical data.\n",
    "The process of using pandas to calculate Pearson's correlation coefficients between pairs of features, helping to identify which features move together directionally.\n",
    "For example, to drop redundant columns from a dataset, you used the following code:\n",
    "\n",
    "### Create a list of redundant column names to drop\n",
    "\n",
    "to_drop = [\"category_desc\", \"created_date\", \"locality\", \"region\", \"vol_requests\"]\n",
    "\n",
    "### Drop those columns from the dataset\n",
    "\n",
    "volunteer_subset = volunteer.drop(to_drop, axis=1)\n",
    "\n",
    "### Print out the head of volunteer_subset\n",
    "\n",
    "print(volunteer_subset.head())\n",
    "\n",
    "This lesson emphasized the iterative nature of feature selection and the need to reassess choices based on model performance.\n",
    "\n",
    "The goal of the next lesson is to understand how to enhance machine learning models by selecting and utilizing the most impactful features from TF-IDF vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb846fe0-b722-402c-9140-5089045c4bac",
   "metadata": {},
   "source": [
    "# Selecting features using text vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d9cba1-9fdb-4188-8a46-8dc131d2e317",
   "metadata": {},
   "source": [
    "# Exploring text vectors, part 1\n",
    "\n",
    "Let's expand on the text vector exploration method we just learned about, using the volunteer dataset's title tf/idf vectors. In this first part of text vector exploration, we're going to add to that function we learned about in the slides. We'll return a list of numbers with the function. In the next exercise, we'll write another function to collect the top words across all documents, extract them, and then use that list to filter down our text_tfidf vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6500b08-3127-44a0-87fd-e59702ad67f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in the rest of the arguments(Add parameters called original_vocab, for the tfidf_vec.vocabulary_, and top_n.)\n",
    "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
    "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
    "    \n",
    "    # Transform that zipped dict into a series(Call pd.Series() on the zipped dictionary. This will make it easier to operate on.)\n",
    "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
    "    \n",
    "    # Sort the series to pull out the top n weighted words(Use the .sort_values() function to sort the series and slice the index up to top_n words.)\n",
    "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
    "    return [original_vocab[i] for i in zipped_index]\n",
    "\n",
    "# Call the function, setting original_vocab=tfidf_vec.vocabulary_, setting vector_index=8 to grab the 9th row, and setting top_n=3, to grab the top 3 weighted words.\n",
    "print(return_weights(vocab,tfidf_vec.vocabulary_, text_tfidf, 8, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf899902-c897-4a6e-a95d-df4a1a558825",
   "metadata": {},
   "source": [
    "# Exploring text vectors, part 2\n",
    "\n",
    "Using the return_weights() function you wrote in the previous exercise, you're now going to extract the top words from each document in the text vector, return a list of the word indices, and use that list to filter the text vector down to those top words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf0a64c-310f-4b5a-88fa-370b3f9b7617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
    "    filter_list = []\n",
    "    for i in range(0, vector.shape[0]):\n",
    "    \n",
    "        # Call return_weights() to return the top weighted words for that document.\n",
    "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
    "        filter_list.extend(filtered)\n",
    "        \n",
    "    # Return the list in a set, so we don't get duplicate word indices(Call set() on the returned filter_list to remove duplicated numbers)\n",
    "    return (set(filter_list))\n",
    "\n",
    "# Call words_to_filter function, passing in the following parameters: vocab for the vocab parameter, tfidf_vec.vocabulary_ for the original_vocab parameter, text_tfidf for the vector parameter, and 3 to grab the top_n 3 weighted words from each document.\n",
    "filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)\n",
    "\n",
    "# Filter the columns in text_tfidf to only those in filtered_words(pass that filtered_words set into a list to use as a filter for the text vector.)\n",
    "filtered_text = text_tfidf[:, list(filtered_words)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df97897-5c2a-4b25-95fc-70806dc0c4f9",
   "metadata": {},
   "source": [
    "Excellent! In the next exercise, you'll train a model using the filtered vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32efa7e-9c55-4c76-8f05-94aa914b9090",
   "metadata": {},
   "source": [
    "# Training Naive Bayes with feature selection\n",
    "\n",
    "You'll now re-run the Naive Bayes text classification model that you ran at the end of Chapter 3 with our selection choices from the previous exercise: the volunteer dataset's title and category_desc columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b83f43-6f25-4577-a8e6-b304ea00dfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset according to the class distribution of category_desc(Use train_test_split() on the filtered_text text vector, the y labels (which is the category_desc labels), and pass the y set to the stratify parameter, since we have an uneven class distribution.)\n",
    "X_train, X_test, y_train, y_test = train_test_split(filtered_text.toarray(), volunteer[\"category_desc\"], stratify=y, random_state=42)\n",
    "\n",
    "# Fit the nb Naive Bayes model to X_train and y_train\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Print out the model's accuracy(ie the test set accuracy of nb)\n",
    "print(nb.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abcf7f9-2c1b-41e4-abcb-34d3aa49e123",
   "metadata": {},
   "source": [
    "You can see that our accuracy score wasn't that different from the score at the end of Chapter 3. But don't worry, this is mainly because of how small the title field is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750c1a50-8388-4c57-9b9d-b09daa62fa42",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19992f40-ed60-4ac2-a5dd-2775cbf5d7da",
   "metadata": {},
   "source": [
    "# Using PCA\n",
    "\n",
    "In this exercise, you'll apply PCA to the wine dataset, to see if you can increase the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9268344e-9dcf-4e05-8704-ec76e351d34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a PCA object\n",
    "pca = PCA()\n",
    "\n",
    "# Define the features (X) and labels (y) from wine, using the labels in the \"Type\" column.\n",
    "X = wine.drop([\"Type\"], axis=1)\n",
    "y = wine[\"Type\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Apply PCA to X_train and X_test, ensuring no data leakage, and store the transformed values as pca_X_train and pca_X_test.\n",
    "pca_X_train = pca.fit_transform(X_train)\n",
    "pca_X_test = pca.transform(X_test)\n",
    "\n",
    "# Print out the .explained_variance_ratio_ attribute of pca to check how much variance is explained by each component.\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5752009-bce1-4285-956c-7219eee437f2",
   "metadata": {},
   "source": [
    "In the next exercise, you'll train a model using the PCA-transformed vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f341d84a-1970-43fb-8ac3-f6c864d540e8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Training a model with PCA\n",
    "\n",
    "Now that you have run PCA on the wine dataset, you'll finally train a KNN model using the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5752f71c-97b3-49ae-a324-97a48b836afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the knn model to the PCA-transformed features, pca_X_train, and training labels, y_train.\n",
    "knn.fit(pca_X_train, y_train)\n",
    "\n",
    "# Print the test set accuracy of the knn model using pca_X_test and y_test\n",
    "print(knn.score(pca_X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01feed1e-87ce-42f3-8ade-0adf3e2ff97c",
   "metadata": {},
   "source": [
    "# Good work! PCA turned out to be a good choice for the wine dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84497af-c88e-4277-8929-99ba1ee9da99",
   "metadata": {},
   "source": [
    "# Putting It All Together (UFOs dataset)\n",
    "\n",
    "Now that you've learned all about preprocessing you'll try these techniques out on a dataset that records information on UFO sightings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83909c3-d46d-4bb3-96ff-e6bac081e86d",
   "metadata": {},
   "source": [
    "## Checking column types\n",
    "\n",
    "Take a look at the UFO dataset's column types using the .info() method. Two columns jump out for transformation: the seconds column, which is a numeric column but is being read in as object, and the date column, which can be transformed into the datetime type. That will make our feature engineering efforts easier later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26f3733-7422-45eb-befb-14870fc9d71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the .info() method on the ufo dataset.\n",
    "print(ufo.info())\n",
    "\n",
    "# Convert the type of the seconds column to the float data type.\n",
    "ufo[\"seconds\"] = ufo[\"seconds\"].astype(float)\n",
    "\n",
    "# Convert the type of the date column to the datetime data type.\n",
    "ufo[\"date\"] = pd.to_datetime(ufo[\"date\"])\n",
    "\n",
    "# Call .info() on ufo again to see if the changes worked.\n",
    "print(ufo.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6639f4eb-8572-49a1-bb3e-11c32b398cf9",
   "metadata": {},
   "source": [
    "Nice job on transforming the column types! This will make feature engineering and standardization much easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704da32e-3d1e-447c-8d4e-1a7c395200f4",
   "metadata": {},
   "source": [
    "## Dropping missing data\n",
    "\n",
    "In this exercise, you'll remove some of the rows where certain columns have missing values. You're going to look at the length_of_time column, the state column, and the type column. You'll drop any row that contains a missing value in at least one of these three columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08729f01-ebd2-4531-bc9f-f18baeef5c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the missing values in the length_of_time, state, and type columns, in that order\n",
    "print(ufo[[\"length_of_time\", \"state\", \"type\"]].isna().sum())\n",
    "\n",
    "# Drop rows where length_of_time, state, or type are missing(The subset parameter is actually used with the dropna() method, which is designed to remove rows with missing values in specific columns)\n",
    "ufo_no_missing = ufo.dropna(subset=[\"length_of_time\", \"state\", \"type\"])\n",
    "\n",
    "# Print out the shape of the new ufo_no_missing dataset.\n",
    "print(ufo_no_missing.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fce917-a36b-43da-b239-2b3a1cab63f8",
   "metadata": {},
   "source": [
    "# Extracting numbers from strings\n",
    "\n",
    "The length_of_time field in the UFO dataset is a text field that has the number of minutes within the string. Here, you'll extract that number from that text field using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec4e80c-7ac2-4dc5-afaa-1d7e3efe3583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_minutes(time_string):\n",
    "\n",
    "    # Search for numbers in time_string\n",
    "    num = re.search(\"\\d+\", time_string)\n",
    "    if num is not None:\n",
    "        return int(num.group(0))\n",
    "        \n",
    "# Use the .apply() method to call the return_minutes() on every row of the length_of_time column.\n",
    "ufo[\"minutes\"] = ufo[\"length_of_time\"].apply(return_minutes)\n",
    "\n",
    "# Print out the .head() of both the length_of_time and minutes columns to compare.\n",
    "print(ufo[[\"minutes\",\"length_of_time\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9f7e23-b647-4b1b-88a0-2d130095befa",
   "metadata": {},
   "source": [
    "## Identifying features for standardization\n",
    "\n",
    "In this exercise, you'll investigate the variance of columns in the UFO dataset to determine which features should be standardized. After taking a look at the variances of the seconds and minutes column, you'll see that the variance of the seconds column is extremely high. Because seconds and minutes are related to each other (an issue we'll deal with when we select features for modeling), let's log normalize the seconds column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1e46d3-4ff7-4b84-9c7d-16a4164e6c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the variance in the seconds and minutes columns and take a close look at the results.\n",
    "print(ufo[[\"minutes\",\"seconds\"]].var())\n",
    "\n",
    "# Perform log normalization on the seconds column, transforming it into a new column named seconds_log\n",
    "ufo[\"seconds_log\"] = np.log(ufo[\"seconds\"])\n",
    "\n",
    "# Print out the variance of just the seconds_log column\n",
    "print(ufo[\"seconds_log\"].var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2383d6-c8ff-4b53-ab92-bff37d9b3939",
   "metadata": {},
   "source": [
    "# Your recent learnings\n",
    "When you left 6 hours ago, you worked on Putting It All Together, chapter 5 of the course Preprocessing for Machine Learning in Python. Here is what you covered in your last lesson:\n",
    "\n",
    "You learned about handling categorical variables and standardizing numerical data in the context of preprocessing the UFO dataset. Specifically, you focused on:\n",
    "\n",
    "One Hot Encoding Categorical Variables: You discovered that categorical variables, such as location data and the type of encounter in the UFO dataset, can be transformed into a format that's suitable for modeling through one hot encoding. This was achieved using pandas' get_dummies function, allowing models to interpret these categorical variables effectively.\n",
    "\n",
    "Extracting Numbers from Strings: You tackled the challenge of extracting numerical values from the length_of_time field, which contains the duration of each sighting in a text format. By employing regular expressions, you extracted the number of minutes from these strings and applied this transformation across the dataset using the .apply() method. Here's how you did it:\n",
    "\n",
    "def return_minutes(time_string):\n",
    "\n",
    "    num = re.search(\"\\d+\", time_string)\n",
    "    \n",
    "    if num is not None:\n",
    "    \n",
    "        return int(num.group(0))\n",
    "        \n",
    "ufo[\"minutes\"] = ufo[\"length_of_time\"].apply(return_minutes)\n",
    "\n",
    "Standardizing Numerical Features: You learned the importance of standardization in preprocessing, particularly for the seconds column of the UFO dataset, which exhibited high variance. By calculating the variance and applying log normalization using NumPy's log function, you transformed the seconds column into a more model-friendly seconds_log column, significantly reducing its variance and making the data more uniform for analysis.\n",
    "\n",
    "These steps are crucial in preparing your dataset for more accurate and efficient model training by ensuring that both categorical and numerical variables are in a suitable format for analysis.\n",
    "\n",
    "The goal of the next lesson is to learn about advanced data preprocessing techniques to further enhance the performance of machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b17caa6-6e59-4b35-b681-8af81a9f91d5",
   "metadata": {},
   "source": [
    "# Engineering new features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bde8d6-134c-4991-8803-3b23887f818f",
   "metadata": {},
   "source": [
    "# Encoding categorical variables\n",
    "\n",
    "There are couple of columns in the UFO dataset that need to be encoded before they can be modeled through scikit-learn. You'll do that transformation here, using both binary and one-hot encoding methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db891044-493b-4054-ac6f-418b86b494db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using apply(), write a conditional lambda function that returns a 1 if the value is \"us\", else return 0.\n",
    "ufo[\"country_enc\"] = ufo[\"country\"].apply(lambda x: 1 if x == \"us\" else 0)\n",
    "\n",
    "# Print out the number of .unique() values in the type column.\n",
    "print(len(ufo[\"type\"].unique()))\n",
    "\n",
    "# Using pd.get_dummies(), create a one-hot encoded set of the type column.\n",
    "type_set = pd.get_dummies(ufo[\"type\"])\n",
    "\n",
    "# Finally, use pd.concat() to concatenate the type_set encoded variables to the ufo dataset.\n",
    "ufo = pd.concat([ufo, type_set], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141d69b6-5374-4828-9e0a-70135fb9eef0",
   "metadata": {},
   "source": [
    "# Features from dates\n",
    "\n",
    "Another feature engineering task to perform is month and year extraction. Perform this task on the date column of the ufo dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4417c2f9-3520-4b92-b022-60d6c660b4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first 5 rows of the date column\n",
    "print(ufo[\"date\"].head())\n",
    "\n",
    "# Extract the month from the date column\n",
    "ufo[\"month\"] = ufo[\"date\"].dt.month\n",
    "\n",
    "# Extract the year from the date column\n",
    "ufo[\"year\"] = ufo[\"date\"].dt.year\n",
    "\n",
    "# Take a look at the .head() of the date, month, and year columns.\n",
    "print(ufo[[\"date\", \"month\", \"year\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd5ef58-cd99-44c4-8e1d-1f09b72cb19a",
   "metadata": {},
   "source": [
    "# Text vectorization\n",
    "\n",
    "You'll now transform the desc column in the UFO dataset into tf/idf vectors, since there's likely something we can learn from this field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d000271-90bd-4bfb-b85d-b208e041281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the head of the desc field\n",
    "print(ufo[\"desc\"].head())\n",
    "\n",
    "# Instantiate the tfidf vectorizer object\n",
    "vec = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the desc column using vec\n",
    "desc_tfidf = vec.fit_transform(ufo[\"desc\"])\n",
    "\n",
    "# Print out the .shape of the desc_tfidf vector, to take a look at the number of columns this created.\n",
    "print(desc_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50029fc-7884-4765-9d9a-6d1ba3853903",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "\n",
    "    0    It was a large&#44 triangular shaped flying ob...\n",
    "        \n",
    "    1    Dancing lights that would fly around and then ...\n",
    "        \n",
    "    2    Brilliant orange light or chinese lantern at o...\n",
    "        \n",
    "    3    Bright red light moving north to north west fr...\n",
    "        \n",
    "    4    North-east moving south-west. First 7 or so li...\n",
    "        \n",
    "    Name: desc, dtype: object\n",
    "        \n",
    "    (1866, 3422)\n",
    "\n",
    "    You'll notice that the text vector has a large number of columns. We'll work on selecting the features we want to use for modeling in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50d60c5-2217-4cc9-a358-b7e134a4ba4c",
   "metadata": {},
   "source": [
    "# Selecting the ideal dataset\n",
    "\n",
    "Now to get rid of some of the unnecessary features in the ufo dataset. Because the country column has been encoded as country_enc, you can select it and drop the other columns related to location: city, country, lat, long, and state.\n",
    "\n",
    "You've engineered the month and year columns, so you no longer need the date or recorded columns. You also standardized the seconds column as seconds_log, so you can drop seconds and minutes.\n",
    "\n",
    "You vectorized desc, so it can be removed. For now you'll keep type.\n",
    "\n",
    "You can also get rid of the length_of_time column, which is unnecessary after extracting minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e6e2d3-cf56-4894-9035-e05d817f6596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of features to drop  \n",
    "to_drop = [\"city\", \"country\", \"date\", \"desc\", \"lat\", \"length_of_time\", \"long\", \"minutes\", \"recorded\", \"seconds\", \"state\"]\n",
    "\n",
    "# Drop these columns from ufo\n",
    "ufo_dropped = ufo.drop(to_drop, axis =1)\n",
    "\n",
    "# Use the words_to_filter() function you created previously; pass in vocab, vec.vocabulary_, desc_tfidf, and keep the top 4 words as the last parameter.\n",
    "filtered_words = words_to_filter(vocab, vec.vocabulary_, desc_tfidf, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cee7521-ea9f-408a-aa60-d28828153c41",
   "metadata": {},
   "source": [
    "# Modeling the UFO dataset, part 1\n",
    "\n",
    "In this exercise, you're going to build a k-nearest neighbor model to predict which country the UFO sighting took place in. The X dataset contains the log-normalized seconds column, the one-hot encoded type columns, as well as the month and year when the sighting took place. The y labels are the encoded country column, where 1 is \"us\" and 0 is \"ca\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec99c21a-6cea-4d36-b4fd-d51c28b7ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the features in the X set of data(Print out the .columns of the X set)\n",
    "print(X.columns)\n",
    "\n",
    "# Split the X and y sets, ensuring that the class distribution of the labels is the same in the training and tests sets, and using a random_state of 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state = 42)\n",
    "\n",
    "# Fit knn to the training data.\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "# Print the test set accuracy of the knn model.\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d806acb3-5593-4e13-95f7-2a0cf2216f8e",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "\n",
    "    Index(['seconds_log', 'changing', 'chevron', 'cigar', 'circle', 'cone', 'cross', 'cylinder', 'diamond', 'disk', 'egg', 'fireball', 'flash', 'formation', 'light', 'other', 'oval', 'rectangle',\n",
    "           'sphere', 'teardrop', 'triangle', 'unknown', 'month', 'year'],\n",
    "          dtype='object')\n",
    "          \n",
    "    0.867237687366167\n",
    "\n",
    "Awesome work! This model performs pretty well. It seems like you've made pretty good feature selection choices here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bc5d3a-fd2a-405f-8c4f-5ec0ac853ab6",
   "metadata": {},
   "source": [
    "# Modeling the UFO dataset, part 2\n",
    "\n",
    "Finally, you'll build a model using the text vector we created, desc_tfidf, using the filtered_words list to create a filtered text vector. Let's see if you can predict the type of the sighting based on the text. You'll use a Naive Bayes model for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0b4fbd-bf34-4f54-9df9-fd58a55483e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the list of filtered words we created to filter the text vector\n",
    "filtered_text = desc_tfidf[:, list(filtered_words)]\n",
    "\n",
    "# Split the X and y sets using train_test_split, setting stratify=y \n",
    "X_train, X_test, y_train, y_test = train_test_split(filtered_text.toarray(), y, stratify=y, random_state=42)\n",
    "\n",
    "# Fit nb to the training sets\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Print the score of nb on the test sets\n",
    "print(nb.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40758678-acf9-4621-8edd-3b0a0490e730",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "\n",
    "    0.17987152034261242\n",
    "\n",
    "this model performs very poorly on this text data. This is a clear case where iteration would be necessary to figure out what subset of text improves the model, and if perhaps any of the other features are useful in predicting type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb43c94-539c-4f3b-86cd-fe83b7c1a0a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
