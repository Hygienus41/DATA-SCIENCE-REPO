{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23510085-4192-4a68-9c28-5a4d12ebbbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6a803f-53d9-48f2-833e-c6c38fce6893",
   "metadata": {},
   "source": [
    "# Experimental Design Preliminaries\n",
    "Building knowledge in experimental design allows you to test hypotheses with best-practice analytical tools and quantify the risk of your work. You’ll begin your journey by setting the foundations of what experimental design is and different experimental design setups such as blocking and stratification. You’ll then learn and apply visual and analytical tests for normality in experimental data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfbf764-2502-430d-b1bd-e10ba2491357",
   "metadata": {},
   "source": [
    "# Non-random assignment of subjects\n",
    "An agricultural firm is conducting an experiment to measure how feeding sheep different types of grass affects their weight. They have asked for your help to properly set up the experiment. One of their managers has said you can perform the subject assignment by taking the top 250 rows from the DataFrame and that should be fine.\n",
    "\n",
    "Your task is to use your analytical skills to demonstrate why this might not be a good idea. Assign the subjects to two groups using non-random assignment (the first 250 rows) and observe the differences in descriptive statistics.\n",
    "\n",
    "You have received the DataFrame, weights which has a column containing the weight of the sheep and a unique id column.\n",
    "\n",
    "numpy and pandas have been imported as np and pd, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d750a58-121f-410a-8c8a-64d3b10879a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Non-random assignment\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Use DataFrame slicing to put the first 250 rows of weights into group1_non_rand and the remaining into group2_non_rand.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m group1_non_rand \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m250\u001b[39m,:]\n\u001b[0;32m      4\u001b[0m group2_non_rand \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m250\u001b[39m:,:]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Compare descriptive statistics of groups\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Generate descriptive statistics of the two groups and concatenate them into a single DataFrame.\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'weights' is not defined"
     ]
    }
   ],
   "source": [
    "# Non-random assignment\n",
    "# Use DataFrame slicing to put the first 250 rows of weights into group1_non_rand and the remaining into group2_non_rand.\n",
    "group1_non_rand = weights.iloc[0:250,:]\n",
    "group2_non_rand = weights.iloc[250:,:]\n",
    "\n",
    "# Compare descriptive statistics of groups\n",
    "# Generate descriptive statistics of the two groups and concatenate them into a single DataFrame.\n",
    "compare_df_non_rand = pd.concat([group1_non_rand['weight'].describe(), group2_non_rand['weight'].describe()], axis=1)\n",
    "compare_df_non_rand.columns = ['group1', 'group2']\n",
    "\n",
    "# Print to assess\n",
    "print(compare_df_non_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04700f1-921d-4464-975c-16f81c22b194",
   "metadata": {},
   "source": [
    "Those two datasets have a much greater difference in means. It may be that the dataset was sorted before you received it. Presenting these results to the firm will help them understand best-practice group assignment. Hopefully you can now work with them to set up the experiment properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c7a194-37a0-4e09-9c99-fc986a59c7c3",
   "metadata": {},
   "source": [
    "# Random assignment of subjects\n",
    "Having built trust from your last work with the agricultural firm, you have been given the task of properly setting up the experiment.\n",
    "\n",
    "Use your knowledge of best practice experimental design set up to assign the sheep to two even groups of 250 each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d023685-19fa-4ef1-896a-6b8518470c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 250 subjects from the weights DataFrame into a new DataFrame group1 without replacement.\n",
    "group1_random = weights.sample(frac=.5, random_state=42, replace= False) # frac=.5 or n=250\n",
    "\n",
    "# Create second assignment\n",
    "group2_random = weights.drop(group1_random.index)\n",
    "\n",
    "# Compare assignments\n",
    "compare_df_random = pd.concat([group1_random['weight'].describe(), group2_random['weight'].describe()], axis=1)\n",
    "compare_df_random.columns = ['group1', 'group2']\n",
    "print(compare_df_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee46b961-ca78-4f84-9283-be8235306dc5",
   "metadata": {},
   "source": [
    "While there are some differences in these datasets, you can clearly see the mean of the two sets are very close. This best-practice setup will ensure the experiment is on the right path from the beginning. Let's continue building foundational experimental design skills by learning about experimental design setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5b36a0-3224-45bd-b229-7334d88af445",
   "metadata": {},
   "source": [
    "# Your recent learnings\n",
    "11 hours ago, you worked on Experimental Design Preliminaries, chapter 1 of the course Experimental Design in Python. Here is what you covered in your last lesson:\n",
    "You learned about the basics of setting up experiments and the importance of experimental design. Experimental design is crucial for making precise and objective conclusions about hypotheses. Here are the key points you covered:\n",
    "•\tTerminology:\n",
    "\n",
    "o\tSubjects: The entities being experimented on (e.g., people, animals).\n",
    "\n",
    "o\tTreatment Group: The group receiving the intervention.\n",
    "\n",
    "o\tControl Group: The group not receiving the intervention, often given a placebo.\n",
    "\n",
    "•\tAssignment Methods:\n",
    "o\tNon-Random Assignment: Splitting subjects into groups without randomization can lead to significant differences between groups, making it harder to attribute changes to the treatment.\n",
    "o\tRandom Assignment: Using randomization to assign subjects to groups ensures that any observed changes are likely due to the treatment rather than inherent differences.\n",
    "•\tPractical Application:\n",
    "o\tYou used pandas to perform both non-random and random assignments of subjects in a DataFrame and compared their descriptive statistics to observe differences.\n",
    "Here's a code snippet demonstrating random assignment:\n",
    "\n",
    "## Randomly assign half\n",
    "\n",
    "group1_random = weights.sample(n=250, random_state=42, replace=False)\n",
    "\n",
    "## Create second assignment\n",
    "\n",
    "group2_random = weights.drop(group1_random.index)\n",
    "\n",
    "## Compare assignments\n",
    "\n",
    "compare_df_random = pd.concat([group1_random['weight'].describe(), group2_random['weight'].describe()], axis=1)\n",
    "compare_df_random.columns = ['group1', 'group2']\n",
    "print(compare_df_random)\n",
    "\n",
    "This exercise highlighted the importance of random assignment in experimental design to ensure reliable and valid results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15527b9b-fc68-4c14-b79f-5407c117741b",
   "metadata": {},
   "source": [
    "# Blocking experimental data\n",
    "You are working with a manufacturing firm that wants to conduct some experiments on worker productivity. Their dataset only contains 100 rows, so it's important that experimental groups are balanced.\n",
    "\n",
    "This sounds like a great opportunity to use your knowledge of blocking to assist them. They have provided a productivity_subjects DataFrame. Split the provided dataset into two even groups of 50 entries each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422af06d-15ca-469f-b4fc-9d9a5f2f0353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select 50 subjects from the productivity_subjects DataFrame into a new DataFrame block_1 without replacement.\n",
    "block_1 = productivity_subjects.sample(50, random_state=42, replace=False)\n",
    "\n",
    "# Set a new column, block to 1 for the block_1 DataFrame.\n",
    "block_1['block'] = 1\n",
    "\n",
    "# Assign the remaining subjects to a DataFrame called block_2 and set the block column to 2 for this DataFrame.\n",
    "block_2 = productivity_subjects.drop(block_1.index)\n",
    "block_2['block'] = 2\n",
    "\n",
    "# Concatenate the blocks together into a single DataFrame, and print the count of each value in the block column to confirm the blocking worked.\n",
    "productivity_combined = pd.concat([block_1, block_2], axis=0)\n",
    "print(productivity_combined['block'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ef02ce-d097-4e7e-ac57-6d853f6d4106",
   "metadata": {},
   "source": [
    "This is important especially when the size of the data is small, as is the case with this data. Let's consider an example where they may be a confounding variable and use stratification to assist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814c0f3d-543c-4f9a-95de-3400789e833d",
   "metadata": {},
   "source": [
    "# Visual normality in an agricultural experiment\n",
    "You have been contracted by an agricultural firm conducting an experiment on 50 chickens, divided into four groups, each fed a different diet. Weight measurements were taken every second day for 20 days.\n",
    "\n",
    "You'll analyze chicken_data to assess normality, which will determine the suitability of parametric statistical tests, beginning with a visual examination of the data distribution. The necessary packages for analysis have been imported for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "192158f4-cbd1-488f-88ad-32ee29706f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from scipy.stats.distributions import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4640d374-075d-4fda-af7b-50c0acbed1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the chickens' weight using the kernel density estimation (KDE) to visualize normality.\n",
    "\n",
    "sns.displot(data=chicken_data, x='weight', kind=\"kde\", dist= norm) # (dist= norm) is optional\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a31b99-ddf5-4998-9f95-5e519dd62040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a qq plot with a standard line of the chickens' weight to assess normality visually.\n",
    "qqplot(data=chicken_data[\"weight\"], line='s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70f4d33-ff4a-4433-91c6-968df0883f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset chicken_data for a 'Time' of 2, and plot the KDE of 'weight' from subset_data to check if data is normal across time.\n",
    "subset_data = chicken_data[chicken_data['Time'] == 2]\n",
    "\n",
    "sns.displot(data=subset_data, x='weight', kind=\"kde\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a4ec64-88bd-4d4b-b15a-fa4eab4aeae0",
   "metadata": {},
   "source": [
    "Your first distribution plot looked a bit normal, but the qq plot was not aligned, and tapered off at the top and bottom. This indicates the data may have tails that affect normality. It also looked a bit more normal at the second time point. Let's confirm some of our thoughts using analytical methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45be2a15-c4ff-4b2e-9114-872f28c9705e",
   "metadata": {},
   "source": [
    "# Analytical normality in an agricultural experiment\n",
    "Carrying on from your previous work, your visual inspections of the data indicate it may not be a normal dataset overall, but that the initial time point may be.\n",
    "\n",
    "Build on your previous work by using analytical methods to determine the normality of the dataset.\n",
    "\n",
    "# Note: when the p-value is less than alpha in a Shapiro-Wilk test, you would not assume normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b459b9a-4b68-4483-b27b-c565178b4f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import anderson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842b53d8-56ea-4644-a7b4-ab57445baf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a Shapiro-Wilk test of normality on the 'weight' column and print the test statistic and p-value.\n",
    "test_statistic, p_value = shapiro(chicken_data[\"weight\"])\n",
    "\n",
    "print(f\"p: {(p_value, 4)} test stat: {round(test_statistic, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f3bab3-048d-4465-bfc9-26574611619f",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "\n",
    "    p: 0.0 test stat: 0.9154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e5cad7-b9a3-49d7-a2a7-c5d81580282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run an Anderson-Darling test for normality and print out the test statistic, significance levels, and critical values from the returned object.\n",
    "result = anderson(chicken_data['weight'], dist=\"norm\")\n",
    "\n",
    "print(f\"Test statistic: {round(result.statistic, 4)}\")\n",
    "print(f\"Significance Levels: {result.significance_level}\")\n",
    "print(f\"Critical Values: {result.critical_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd1d9f3-de0f-47ef-8056-d8e56856a66f",
   "metadata": {},
   "source": [
    "# output\n",
    "<script.py> output:\n",
    "\n",
    "    Test statistic: 12.\n",
    "    \n",
    "    Significance Levels: [15.  10.   5.   2.5  1. ]\n",
    "    \n",
    "    Critical Values: [0.572 0.651 0.781 0.911 1.084]\n",
    "\n",
    "The critical value which matches the significance level of 5 is 0.781. When compared to the Anderson-Darling test statistic (12.5451), the critical value is much smaller and so we reject the null hypothesis and can conclude the data is unlikely to have been drawn from a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d619966b-b098-42e9-97f8-e00a7ed709fe",
   "metadata": {},
   "source": [
    "#  Chapter 2: Experimental Design Techniques\n",
    "Delve into sophisticated experimental design techniques, focusing on factorial designs, randomized block designs, and covariate adjustments. These methodologies are instrumental in enhancing the accuracy, efficiency, and interpretability of experimental results. Through a combination of theoretical insights and practical applications, you'll acquire the skills needed to design, implement, and analyze complex experiments in various fields of research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929eb14d-e2ac-403d-9fdb-e8b347349047",
   "metadata": {},
   "source": [
    "# Understanding marketing campaign effectiveness\n",
    "Imagine you're a digital marketer analyzing data from a recent campaign to understand what messaging style and time of day yield the highest conversions. This analysis is crucial for guiding future marketing strategies, ensuring that your messages reach potential customers when they're most likely to engage. In this exercise, you're working with a dataset giving the outcomes of different messaging styles ('Casual' versus 'Formal') and times of day ('Morning' versus 'Evening') on conversion rates, a common scenario in marketing data analysis.\n",
    "\n",
    "The data has been loaded for you as a DataFrame named marketing_data, and pandas is loaded as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e334af5-a0da-4e09-8273-e6c2d484eaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table with 'Messaging_Style' as the index and 'Time_of_Day' as the columns, computing the mean of Conversions.\n",
    "marketing_pivot = marketing_data.pivot_table(\n",
    "  values='Conversions', \n",
    "  index='Messaging_Style', \n",
    "  columns='Time_of_Day', \n",
    "  aggfunc='mean')\n",
    "\n",
    "# View the pivoted results\n",
    "print(marketing_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e50aeff-c0df-426d-a2e8-a8042ed53e66",
   "metadata": {},
   "source": [
    "\n",
    "<script.py> output:\n",
    "\n",
    "    Time_of_Day      Evening  Morning\n",
    "\n",
    "    Messaging_Style     \n",
    "\n",
    "    Casual           402.329  401.134\n",
    "\n",
    "    Formal           432.913  411.096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b48c3a-d3a2-435b-8a0f-3c5d41605024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize interactions between Messaging_Style and Time_of_Day with respect to conversions by creating an annotated cool-warm heatmap of \n",
    "# marketing_pivot.\n",
    "sns.heatmap(marketing_pivot, \n",
    "            annot=True, \n",
    "            cmap='coolwarm',\n",
    "            fmt='g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d767d492-cbeb-48f7-b2d7-58ebaf85c0cf",
   "metadata": {},
   "source": [
    "Factorial designs and randomized block designs\n",
    "Select the three correct statements regarding factorial designs and randomized block designs.\n",
    "\n",
    "Factorial designs require each experimental unit to be exposed to all possible combinations of treatment levels\n",
    "\n",
    "Randomized block designs enhance experimental precision by controlling for variability within groups of similar subjects\n",
    "\n",
    "Factorial designs are particularly useful in complex scenarios with multiple factors influencing the outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c5d7c6-2cdb-493b-8e1c-23907f8924bd",
   "metadata": {},
   "source": [
    "# Your recent learnings\n",
    "When you left 1 day ago, you worked on Experimental Design Techniques, chapter 2 of the course Experimental Design in Python. Here is what you covered in your last lesson:\n",
    "\n",
    "You learned about factorial designs, which allow you to examine multiple variables simultaneously and their interactions. This method tests every possible combination of factor levels, providing insights into complex dynamics that simpler setups might miss. Here's a recap of the key points:\n",
    "\n",
    "Factorial Designs: These designs test all combinations of factor levels to measure both direct effects and interactions. For example, in a plant growth experiment, you can test the effects of light conditions and fertilizer types simultaneously.\n",
    "Pivot Tables: You created a pivot table using pandas to aggregate data. For instance, you used pivot_table to calculate the mean growth for each combination of light condition and fertilizer type.\n",
    "\n",
    "plant_pivot = plant_data.pivot_table(\n",
    "   \n",
    "    values='Growth_cm', \n",
    "    \n",
    "    index='Light_Condition', \n",
    "    \n",
    "    columns='Fertilizer_Type', \n",
    "    \n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "print(plant_pivot)\n",
    "\n",
    "Heatmaps: You visualized interactions using Seaborn's heatmap function, which helps identify how different factors interact by showing the intensity of their effects.\n",
    "\n",
    "Comparing Designs: Factorial designs explore multiple treatments and their interactions, while randomized block designs group similar subjects to minimize confounding impacts and enhance precision.\n",
    "\n",
    "These techniques are crucial for designing, implementing, and analyzing complex experiments effectively.\n",
    "\n",
    "The goal of the next lesson is to understand how to use randomized block design to control for variance and improve the precision of experimental results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4623595-11f0-4c10-8508-69f5c72c44de",
   "metadata": {},
   "source": [
    "# Implementing a randomized block design\n",
    "The manufacturing firm you worked with earlier is still interested in conducting some experiments on worker productivity. Previously, the two blocks were set randomly. While this can work, it can be better to group subjects based on similar characteristics.\n",
    "\n",
    "The same employees are again loaded but this time in a DataFrame called productivity including 1200 other colleagues. It also includes a worker 'productivity_score' column based on units produced per hour. This column was binned into three groups to generate blocks based on similar productivity values. The firm would like to apply a new incentive program with three options ('Bonus', 'Profit Sharing' and 'Work from Home') throughout the firm with treatment applied randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a4a4f4-8888-4429-b91f-1b961566ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly assign workers to blocks(Shuffle the blocks to create a new DataFrame called prod_df.)\n",
    "prod_df = productivity.groupby('block').apply(\n",
    "  lambda x: x.sample(frac=1)\n",
    ")\n",
    "\n",
    "# Reset the index(Reset the index so that block is not both an index and a column.)\n",
    "prod_df = prod_df.reset_index(drop=True)\n",
    "\n",
    "# Randomly assign the three treatment values in the 'Treatment' column.\n",
    "prod_df['Treatment'] = np.random.choice(\n",
    "  ['Bonus', 'Profit Sharing', 'Work from Home'],\n",
    "  size=len(prod_df)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614c11f5-9693-46d2-8c93-281ea908be3e",
   "metadata": {},
   "source": [
    "You've efficiently shuffled workers within their blocks, streamlined the DataFrame by resetting the index, and randomly assigned treatments, setting a solid foundation for analyzing incentive effects on productivity. Your skills in preparing data for experimental analysis are on point!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1ebb75-637d-46aa-bb9e-494f9febba43",
   "metadata": {},
   "source": [
    "# Visualizing productivity within blocks by incentive\n",
    "Continuing with the worker productivity example, you'll explore if the productivity scores are distributed throughout the data as one would expect with random assignment of treatment. Note that this is a precautionary step, and the treatment and follow-up results on the impact of the three treatments is not done yet!\n",
    "\n",
    "seaborn and matplotlib.pyplot as sns and plt respectively are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28258e5c-65b0-45ef-8173-8123afd31179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the productivity scores within blocks by treatment using a boxplot with 'block' for x, 'productivity_score' for y, and 'Treatment' for hue.\n",
    "sns.boxplot(x='block', \n",
    "            y='productivity_score', \n",
    "            hue='Treatment', \n",
    "            data=prod_df)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e418411c-6cf7-4467-b720-6b2a7750ea8a",
   "metadata": {},
   "source": [
    "You've successfully created a visualization that illustrates how the 'productivity_score' varies within different blocks, with the additional layer of treatment differentiation. Notice that the 'productivity_score' values vary greatly across blocks—that's how you set up the blocks to start! You, therefore, won't test for variability across blocks, but will you see significant variability within blocks? Time to find out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449bbefa-7765-40da-97b7-97d2a3a284f1",
   "metadata": {},
   "source": [
    "# ANOVA within blocks of employees\n",
    "Building on your previous analyses with the manufacturing firm, where worker productivity was examined across different blocks and an incentive program was introduced, you're now delving deeper into the data. The firm, equipped with a more comprehensive dataset in the productivity DataFrame, including 1200 additional employees and their productivity_score, has structured the workforce into three blocks based on productivity levels. Each employee has been randomly assigned one of three incentive options: 'Bonus', 'Profit Sharing', or 'Work from Home'.\n",
    "\n",
    "Before assessing the full impact of these incentive treatments on productivity, it's crucial to verify that the initial treatment assignment was indeed random and equitable across the different productivity blocks. This step ensures that any observed differences in productivity post-treatment can be confidently attributed to the incentive programs themselves, rather than pre-existing disparities in the blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4684cce0-6bb2-44e7-8114-7f9d60291ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cc2c9d-0cd3-4f1b-89fe-bc7076b2aca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group prod_df by the appropriate column that represents different blocks in your data.\n",
    "within_block_anova = prod_df.groupby('block').apply(\n",
    "# Use a lambda function to apply the ANOVA test within each block, specifying the lambda function's argument.\n",
    "# For each treatment group within the blocks, filter prod_df based on the 'Treatment' column values and select the 'productivity_score' column.\n",
    "  lambda x: f_oneway(\n",
    "    # Filter Treatment values based on outcome\n",
    "    x[x['Treatment'] == 'Bonus']['productivity_score'], \n",
    "    x[x['Treatment'] == 'Profit Sharing']['productivity_score'],\n",
    "    x[x['Treatment'] == 'Work from Home']['productivity_score'])\n",
    ")\n",
    "print(within_block_anova)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110878f7-9636-41d1-80d4-8065ee6999eb",
   "metadata": {},
   "source": [
    "\n",
    "<script.py> output:\n",
    "\n",
    "    block\n",
    "    1    (1.009243027139357, 0.36539191608501026)\n",
    "    \n",
    "    2    (0.09964675193646039, 0.905177667445047)\n",
    "    \n",
    "    3    (0.2983940138555918, 0.7421589537946647)\n",
    "    \n",
    "    dtype: object\n",
    "\n",
    "    You've adeptly conducted an ANOVA analysis across the different blocks, comparing the productivity scores for the three treatment groups. Notice that each of the three p-values are large, so you can feel confident in how this randomized block design is set up as an experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3078c09-e261-426a-b7f9-fd78ff804218",
   "metadata": {},
   "source": [
    "# Importance of covariates\n",
    "Why is it important to include covariates in statistical analyses?\n",
    "\n",
    "To account for potential variability and reduce confounding in the analysis(Covariates help refine your analysis by accounting for additional variability and reducing confounding effects.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d51b2e7-5b29-4c90-88e0-e86080c05c89",
   "metadata": {},
   "source": [
    "# Covariate adjustment with chick growth\n",
    "Imagine studying in agricultural science the growth patterns of chicks under various dietary regimens. The data from this study sheds light on the intricate relationship between their respective diets and the consequent impact on their weight. This data includes weight measurements of chicks at different ages, allowing for an exploration of covariate adjustment. age serves as a covariate, potentially influencing the outcome variable: the weight of the chicks.\n",
    "\n",
    "DataFrames exp_chick_data, the experimental data, and cov_chick_data, the covariate data, have been loaded, along with the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08ee97c0-2111-4fe6-a10e-fafa1430fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.formula.api import ols\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8f90ef-eca3-4f1e-a6e1-dbfedbf85b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the experimental and covariate data based on common column(s), and print this merged data.\n",
    "merged_chick_data = pd.merge(exp_chick_data, \n",
    "                            cov_chick_data, on='Chick')\n",
    "\n",
    "# Print the merged data\n",
    "print(merged_chick_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81aaacf-87ae-4ab8-bbdb-985fef0f04a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform ANCOVA with Diet and Time as predictors(Produce an ANCOVA predicting 'weight' based on 'Diet' and 'Time')\n",
    "model = ols('weight ~ Diet + Time', data=merged_chick_data).fit()\n",
    "\n",
    "# Print a summary of the ANCOVA model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ba37c5-c487-42e0-ac6b-35820a3ca122",
   "metadata": {},
   "source": [
    "\n",
    "<script.py> output:\n",
    "                                OLS Regression Results                            \n",
    "    ==============================================================================\n",
    "    Dep. Variable:                 weight   R-squared:                       0.040\n",
    "    Model:                            OLS   Adj. R-squared:                  0.039\n",
    "    Method:                 Least Squares   F-statistic:                     140.9\n",
    "    Date:                Sun, 16 Nov 2025   Prob (F-statistic):           1.12e-60\n",
    "    Time:                        05:09:32   Log-Likelihood:                -38608.\n",
    "    No. Observations:                6818   AIC:                         7.722e+04\n",
    "    Df Residuals:                    6815   BIC:                         7.724e+04\n",
    "    Df Model:                           2                                         \n",
    "    Covariance Type:            nonrobust                                         \n",
    "    ==============================================================================\n",
    "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
    "    ------------------------------------------------------------------------------\n",
    "    Intercept     94.0678      2.275     41.342      0.000      89.607      98.528\n",
    "    Diet          12.2022      0.729     16.747      0.000      10.774      13.631\n",
    "    Time           0.1238      0.125      0.990      0.322      -0.121       0.369\n",
    "    ==============================================================================\n",
    "    Omnibus:                      694.766   Durbin-Watson:                   0.055\n",
    "    Prob(Omnibus):                  0.000   Jarque-Bera (JB):              922.241\n",
    "    Skew:                           0.886   Prob(JB):                    5.47e-201\n",
    "    Kurtosis:                       3.326   Cond. No.                         35.8\n",
    "    ==============================================================================\n",
    "    \n",
    "    Notes:\n",
    "    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b574d9-7cf2-4775-ba84-a4345b51521a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb9dea-6ed9-4d71-9057-548ddadb14f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design an lmplot to see hue='Diet' effects on y='weight' adjusted for x='Time'.(Visualize Diet effects with Time adjustment)\n",
    "sns.lmplot(x='Time', y='weight', \n",
    "         hue='Diet', \n",
    "         data=merged_chick_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b822d62-3600-42d9-9763-848071c8a1a1",
   "metadata": {},
   "source": [
    "# Your recent learnings\n",
    "When you left 2 days ago, you worked on Experimental Design Techniques, chapter 2 of the course Experimental Design in Python. Here is what you covered in your last lesson:\n",
    "\n",
    "You learned about covariate adjustment in experimental design and its importance in minimizing confounding effects. Covariates are variables related to the outcome variable but not of primary interest. By including them in the analysis, you can isolate the effect of the independent variable on the outcome. Here are the key points you covered:\n",
    "\n",
    "Covariates: These are additional variables that can influence the outcome variable. They are included in the analysis to control for their effects and reduce confounding.\n",
    "\n",
    "ANCOVA (Analysis of Covariance): This technique evaluates treatment effects while controlling for covariates. It helps in isolating the true effect of the independent variable on the dependent variable.\n",
    "\n",
    "Combining DataFrames: You used pandas' merge function to combine experimental data with covariate data, ensuring each subject's data is aligned.\n",
    "\n",
    "Modeling with ANCOVA: You employed the ols model from statsmodels to adjust for covariates. For example:\n",
    "\n",
    "model = ols('Growth_cm ~ Fertilizer_Type + Watering_Days_Per_Week', data=exp_data).fit()\n",
    "print(model.summary())\n",
    "\n",
    "Interpreting Results: You learned to interpret the summary output, focusing on p-values to determine the significance of covariates and treatment effects.\n",
    "Armed with this understanding, you're now ready to apply covariate adjustment in your own analyses.\n",
    "\n",
    "The goal of the next lesson is to understand how to interpret the results of different statistical tests to make informed decisions based on data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec40a45a-9de2-4992-8d44-380da0dadf3a",
   "metadata": {},
   "source": [
    "# Choosing the right test: petrochemicals\n",
    "In a chemistry research lab, scientists are examining the efficiency of three well-known catalysts—Palladium (Pd), Platinum (Pt), and Nickel (Ni)—in facilitating a particular reaction. Each catalyst is used in a set of identical reactions under controlled conditions, and the time taken for each reaction to reach completion is meticulously recorded. Your goal is to compare the mean reaction times across the three catalyst groups to identify which catalyst, if any, has a significantly different reaction time.\n",
    "\n",
    "The data is available in the chemical_reactions DataFrame. pandas as pd, numpy as np, and the following functions have been loaded as well:\n",
    "\n",
    "What type of hypothesis test should be performed in this scenario?\n",
    "\n",
    "One-way ANOVA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "251fb78f-32eb-4c41-9081-733d7dc72a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import f_oneway\n",
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cce891-3e29-41c3-beed-14711a444d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalyst_types = ['Palladium', 'Platinum', 'Nickel']\n",
    "\n",
    "# Use a list comprehension to filter into groups iterating over the catalyst_types and each of their 'Reaction_Time's.\n",
    "groups = [chemical_reactions[chemical_reactions['Catalyst'] == catalyst]['Reaction_Time'] for catalyst in catalyst_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1836fe8c-e980-4ceb-baf8-909321cef098",
   "metadata": {},
   "outputs": [],
   "source": [
    "Perform the one-way ANOVA across the three groups\n",
    "f_stat, p_val = f_oneway(*groups)\n",
    "print(p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fce2e81-f252-4c3d-8a7d-139ad768c168",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "\n",
    "    4.710677600047866e-151\n",
    "\n",
    "Assume a significance level of 0.01. What is the appropriate conclusion to glean from the P-value in comparison with this alpha value?\n",
    "\n",
    "The P-value is substantially smaller than the alpha value, indicating a significant difference in reaction times across the catalysts.\n",
    "The extremely small P-value strongly suggests significant differences among the catalysts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f037f7-eb5b-433f-af87-005113f0d885",
   "metadata": {},
   "source": [
    "# Choosing the right test: human resources\n",
    "In human resources, it's essential to understand the relationships between different variables that might influence employee satisfaction or turnover. Consider a scenario where an HR department is interested in understanding the association between the department in which employees work and their participation in a new workplace wellness program. The HR team has compiled this data over the past two years and has asked you if there's any significant association between an employee's department and their enrolling in the wellness program.\n",
    "\n",
    "The data is available in the hr_wellness DataFrame. pandas as pd, numpy as np, and the following functions have been loaded:\n",
    "\n",
    "What type of hypothesis test should be performed in this scenario?\n",
    "\n",
    "Chi-square test of association"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4f76034-9c30-425a-8a90-a3c308d20136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import f_oneway\n",
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33cb4f7-f998-4f91-bbe4-ad348a66e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a contingency table comparing 'Department' and 'Wellness_Program_Status'.\n",
    "contingency_table = pd.crosstab(\n",
    "  hr_wellness['Department'], \n",
    "  hr_wellness['Wellness_Program_Status']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2535a9-6aed-4233-80fa-e8a819b66de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a chi-square test of association on the contingency table and print the p-value.\n",
    "chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)\n",
    "print(p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b72d9eb-f306-4102-9775-f00617f37baa",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "\n",
    "    0.17573344450112738\n",
    "\n",
    "Assume a significance level of 0.05. Given the P-value, what is the appropriate conclusion?\n",
    "\n",
    "answer:\n",
    "\n",
    "There's no significant association between department and enrollment in the wellness program, as the P-value is larger than 0.05.\n",
    "The P-value being greater than 0.05 suggests no significant association between the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607d3cc5-d98a-4b76-bb0d-b50508570db8",
   "metadata": {},
   "source": [
    "# Choosing the right test: finance\n",
    "In the realm of finance, investment strategists are continually evaluating different approaches to maximize returns. Consider a scenario where a financial firm wishes to assess the effectiveness of two investment strategies: \"Quantitative Analysis\" and \"Fundamental Analysis\". The firm has applied each strategy to a separate set of investment portfolios for a year and now asks you to compare the annual returns to determine if there is any difference in strategy returns by comparing the mean returns of the two groups.\n",
    "\n",
    "The data is available in the investment_returns DataFrame. pandas as pd, numpy as np, and the following functions have been loaded as well:\n",
    "\n",
    "\n",
    "What type of hypothesis test should be performed in this scenario?\n",
    "\n",
    "answers:\n",
    "\n",
    "Independent samples t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44d24529-63d4-4e67-9b73-0c262c812954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import f_oneway\n",
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8481286f-3233-4c50-b6f2-7a6dbc279d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter 'Strategy_Type' on 'Quantitative' to retrieve their 'Annual_Return' and do the same for 'Fundamental' strategies.\n",
    "quantitative_returns = investment_returns[investment_returns['Strategy_Type'] =='Quantitative']['Annual_Return']\n",
    "fundamental_returns = investment_returns[investment_returns['Strategy_Type'] == 'Fundamental']['Annual_Return']\n",
    "\n",
    "print(quantitative_returns, fundamental_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c08c391-046a-49ca-bb1c-328f5cc1c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the independent samples t-test between the two groups(Complete for the two groups an independent samples t-test and print the p-value.)\n",
    "t_stat, p_val = ttest_ind(quantitative_returns, fundamental_returns)\n",
    "print(t_stat,p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7936fdc1-d36a-495d-8993-936b63b61746",
   "metadata": {},
   "source": [
    "Assume a significance level of 0.1. What is the appropriate conclusion to glean from the P-value in comparison with this \n",
    " value?\n",
    "\n",
    "answers\n",
    "\n",
    "The P-value is much smaller than alpha suggesting a significant difference in returns between the two strategies.\n",
    "<script.py> output:\n",
    "\n",
    "    7.784788496693728 2.0567003424807146e-14\n",
    "\n",
    "Given the very small p-value of around 0.000000000000002, we have evidence of a difference in returns for any reasonable choice of alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9731916c-6c78-4e6f-b87b-dc2932bcf84a",
   "metadata": {},
   "source": [
    "# POST HOC ANALYSIS FOLLOWING ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c28650-0648-4d35-a4a6-9f5b7b452abc",
   "metadata": {},
   "source": [
    "# Anxiety treatments ANOVA\n",
    "Psychologists conducted a study to compare the effectiveness of three types of therapy on reducing anxiety levels: Cognitive Behavioral Therapy (CBT), Dialectical Behavior Therapy (DBT), and Acceptance and Commitment Therapy (ACT). Participants were randomly assigned to one of the three therapy groups, and their anxiety levels were measured before and after the therapy sessions. The psychologists have asked you to determine if there are any significant differences in the effectiveness of these therapies.\n",
    "\n",
    "The therapy_outcomes DataFrame containing this experiment data has been loaded along with pandas as pd and from scipy.stats import f_oneway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abc03c6-87a0-409b-bf28-b3a89b943dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot to view the mean anxiety reduction for each therapy (Create a pivot table to calculate the mean 'Anxiety_Reduction' value across groups of 'Therapy_Type' in this data)\n",
    "pivot_table = therapy_outcomes.pivot_table(\n",
    "    values='Anxiety_Reduction', \n",
    "    index='Therapy_Type', \n",
    "    aggfunc=\"mean\")\n",
    "print(pivot_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965ba563-4a37-4914-8555-5ed7a817adff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter groups of therapy types and their 'Anxiety_Reduction' values by first creating a list of the three therapy types: 'CBT', 'DBT', and 'ACT'.\n",
    "# (Create groups to prepare the data for ANOVA)\n",
    "therapy_types = ['CBT', 'DBT', 'ACT']\n",
    "groups = [therapy_outcomes[therapy_outcomes['Therapy_Type'] == therapy]['Anxiety_Reduction'] for therapy in therapy_types]\n",
    "\n",
    "# Conduct a one-way ANOVA\n",
    "f_stat, p_val = f_oneway(*groups)\n",
    "print(p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c7d75f-0f73-4229-b66c-b46f1d608571",
   "metadata": {},
   "source": [
    "\n",
    "<script.py> output:\n",
    "\n",
    "                  Anxiety_Reduction\n",
    "\n",
    "    Therapy_Type \n",
    "\n",
    "    ACT                      14.929\n",
    "\n",
    "    CBT                      14.962\n",
    "\n",
    "    DBT                      15.729\n",
    "\n",
    "    0.019580062979016804\n",
    "\n",
    "By analyzing the data with ANOVA, you've taken an important step in comparing the effectiveness of different therapies. Assuming an alpha of 0.05, the P-value indicates significant differences in therapy effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9415f82d-cdc4-42a8-85a2-6d43e2264e25",
   "metadata": {},
   "source": [
    "# Applying Tukey's HSD\n",
    "Following the ANOVA analysis which suggested significant differences in the effectiveness of the three types of therapy, the psychologists are keen to delve deeper. They wish for you to explain exactly which therapy types differ from each other in terms of reducing anxiety levels. This is where Tukey's Honest Significant Difference (HSD) test comes into play. It's a post-hoc test used to make pairwise comparisons between group means after an ANOVA has shown a significant difference. Tukey's HSD test helps in identifying specific pairs of groups that have significant differences in their means.\n",
    "\n",
    "The therapy_outcomes DataFrame containing this experiment data has again been loaded along with pandas as pd and from statsmodels.stats.multicomp import pairwise_tukeyhsd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed03166-2167-4853-9e44-9a51a040181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At a significance level of 0.05, perform Tukey's HSD test to compare the mean anxiety reduction across the three therapy groups.\n",
    "tukey_results = pairwise_tukeyhsd(   therapy_outcomes['Anxiety_Reduction'], therapy_outcomes['Therapy_Type'], \n",
    "    alpha=0.05\n",
    ")\n",
    "print(tukey_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0680c2-3403-4e79-a9fb-9002e85e0499",
   "metadata": {},
   "source": [
    "<script.py> output:\n",
    "    Multiple Comparison of Means - Tukey HSD, FWER=0.05\n",
    "    ===================================================\n",
    "    group1 group2 meandiff p-adj   lower  upper  reject\n",
    "    ---------------------------------------------------\n",
    "       ACT    CBT    0.033 0.9941 -0.7136 0.7795  False\n",
    "       ACT    DBT   0.8001 0.0358  0.0418 1.5583   True\n",
    "       CBT    DBT   0.7671 0.0433  0.0181 1.5161   True\n",
    "    ---------------------------------------------------\n",
    "\n",
    "\n",
    "The Tukey HSD test provided clear insights into which therapy types significantly differ in reducing anxiety. These findings can guide psychologists in refining treatment approaches. Did you catch that ACT and CBT don't differ significantly from this experiment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98b9492-390a-4d67-9b7b-433ad945d7fe",
   "metadata": {},
   "source": [
    "# Applying Bonferoni correction\n",
    "After identifying significant differences between therapy groups with Tukey's HSD, we want to confirm our findings with the Bonferroni correction. The Bonferroni correction is a conservative statistical adjustment used to counteract the problem of multiple comparisons. It reduces the chance of obtaining false-positive results by adjusting the significance level. In the context of your study on the effectiveness of CBT, DBT, and ACT, applying the Bonferroni correction will help ensure that the significant differences you observe between therapy groups are not due to chance.\n",
    "\n",
    "The therapy_outcomes DataFrame has again been loaded along with pandas as pd, from scipy.stats import ttest_ind, and from statsmodels.sandbox.stats.multicomp import multipletests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75ed388-f5da-43a2-a917-ab00640f46b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct independent t-tests between all pairs of therapy groups in therapy_pairs and append the p-values (p_val) to the p_values list.\n",
    "p_values = []\n",
    "\n",
    "therapy_pairs = [('CBT', 'DBT'), ('CBT', 'ACT'), ('DBT', 'ACT')]\n",
    "\n",
    "# Apply the Bonferroni correction to adjust the p-values from the multiple tests and print them.\n",
    "for pair in therapy_pairs:\n",
    "    group1 = therapy_outcomes[therapy_outcomes['Therapy_Type'] == \"pair\"]['Anxiety_Reduction']\n",
    "    group2 = therapy_outcomes[therapy_outcomes['Therapy_Type'] == \"pair\"]['Anxiety_Reduction']\n",
    "    t_stat, p_val = ttest_ind(group1, group2)\n",
    "    p_values.append(p_val)\n",
    "\n",
    "# Apply Bonferroni correction\n",
    "print(multipletests(p_values, alpha=0.05, method='bonferroni')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d78f92-cebd-41ac-9b01-751e4daeefc6",
   "metadata": {},
   "source": [
    "You've adeptly applied the Bonferroni correction to adjust the P-values for multiple comparisons. This step is critical to control for Type I error, ensuring the reliability of your findings. Here, you again see that ACT and CBT don't differ significantly from this experiment due to the corrected P-value of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb65d69-996a-4e28-a664-838a17747df4",
   "metadata": {},
   "source": [
    "# Your recent learnings\n",
    "When you left 20 hours ago, you worked on Analyzing Experimental Data: Statistical Tests and Power, chapter 3 of the course Experimental Design in Python. Here is what you covered in your last lesson:\n",
    "\n",
    "You learned about post-hoc analysis following ANOVA, which helps identify specific differences between groups after ANOVA indicates significant differences. Here's a recap of the key points:\n",
    "\n",
    "Post-hoc Analysis: Essential for understanding pairwise differences after ANOVA.\n",
    "\n",
    "Tukey's HSD: Robust for multiple comparisons, useful for broader comparisons.\n",
    "Bonferroni Correction: Adjusts p-values to control for Type I errors, ideal for focused tests.\n",
    "Practical Application:\n",
    "\n",
    "ANOVA: Used to assess significant differences in Click_Through_Rates among different Ad campaigns.\n",
    "Tukey's HSD Test:\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "tukey_results = pairwise_tukeyhsd(\n",
    "    therapy_outcomes['Anxiety_Reduction'], \n",
    "    therapy_outcomes['Therapy_Type'], \n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "print(tukey_results)\n",
    "Bonferroni Correction:\n",
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "\n",
    "p_values = []\n",
    "therapy_pairs = [('CBT', 'DBT'), ('CBT', 'ACT'), ('DBT', 'ACT')]\n",
    "\n",
    "for pair in therapy_pairs:\n",
    "    group1 = therapy_outcomes[therapy_outcomes['Therapy_Type'] == pair[0]]['Anxiety_Reduction']\n",
    "    group2 = therapy_outcomes[therapy_outcomes['Therapy_Type'] == pair[1]]['Anxiety_Reduction']\n",
    "    t_stat, p_val = ttest_ind(group1, group2)\n",
    "    p_values.append(p_val)\n",
    "\n",
    "print(multipletests(p_values, alpha=0.05, method='bonferroni')[1])\n",
    "These methods ensure you can accurately identify and confirm significant differences between groups in your data.\n",
    "\n",
    "The goal of the next lesson is to learn how to conduct power analysis to determine the sample size needed for detecting a significant effect in your experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92458084-a3cf-417e-b6c9-1facd7263458",
   "metadata": {},
   "source": [
    "# Analyzing toy durability\n",
    "In product development within the toy industry, it's crucial to understand the durability of toys, particularly when comparing educational toys to recreational ones. Durability can significantly impact customer satisfaction and repeat business. Researchers in a toy manufacturing company have asked you to conduct the analysis of a study comparing the durability of educational toys versus recreational toys. The toy_durability DataFrame contains the results of these tests, with durability scores assigned based on rigorous testing protocols.\n",
    "\n",
    "The data is available in the toy_durability DataFrame. pandas as pd and from scipy.stats import ttest_ind have been loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38ecfc9-0024-470e-8c15-1f1213217957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean 'Durability_Score' for both 'Educational' and 'Recreational' toys using a pivot table.\n",
    "mean_durability = toy_durability.pivot_table(\n",
    "  values='Durability_Score', index='Toy_Type', aggfunc=\"mean\")\n",
    "print(mean_durability)\n",
    "\n",
    "# Perform an independent samples t-test to compare the durability of 'Educational' and 'Recreational' toys by first separating durability scores by Toy_Type.\n",
    "educational_durability = toy_durability[toy_durability['Toy_Type'] == 'Educational']['Durability_Score']\n",
    "recreational_durability = toy_durability[toy_durability['Toy_Type'] == 'Recreational']['Durability_Score']\n",
    "t_stat, p_val = ttest_ind(recreational_durability, educational_durability)\n",
    "\n",
    "print(p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f349a62-6271-40d0-ae59-f13df2721dce",
   "metadata": {},
   "source": [
    "The P-value suggests that there's a statistically significant difference in durability between 'Educational' and 'Recreational' toys, assuming an alpha of 0.05. This insight could be crucial for product development and marketing strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decc035e-36b4-4040-9aaa-88b2e06ebdc1",
   "metadata": {},
   "source": [
    "# Visualizing durability differences\n",
    "Following the analysis of toy durability, the research team is interested in you visualizing the distribution of durability scores for both Educational and Recreational toys. Such visualizations can offer intuitive insights into the data, potentially highlighting the range and variability of scores within each category. This step is essential for presenting findings to non-technical stakeholders and guiding further product development decisions.\n",
    "\n",
    "The data is available in the toy_durability DataFrame, and seaborn and matplotlib.pyplot as sns and plt respectively are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f8de4e-254a-48c5-9909-52b26326b446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of 'Durability_Score' for Educational and Recreational toys using a Kernel Density Estimate (KDE) plot,\n",
    "# highlighting differences by using the 'Toy_Type' column to color the distributions differently.\n",
    "sns.displot(data=toy_durability, x=\"Durability_Score\", \n",
    "         hue=\"Toy_Type\", kind=\"kde\")\n",
    "plt.title('Durability Score Distribution by Toy Type')\n",
    "plt.xlabel('Durability Score')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d7b716-57a1-49c9-be40-982f670ebacc",
   "metadata": {},
   "source": [
    "The KDE plot visually illustrates the differences in durability between Educational and Recreational toys. You can see that the center of both distributions is near 80 for the durability score, but Recreational seems more variable than Educational.\n",
    "\n",
    "Alpha levels set the probability threshold for rejecting the null hypothesis, reflecting the risk of committing a Type I error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3027c3-7ff1-4452-b31a-d23060345779",
   "metadata": {},
   "source": [
    "# Effect size purpose\n",
    "What is the primary purpose of estimating effect size (such as Cohen's d) in the context of power analysis?\n",
    "\n",
    "To quantify how big the expected difference or relationship is, so you can determine how many participants (sample size) you need to reliably detect that effect.\n",
    "Why effect size is essential in power analysis\n",
    "\n",
    "Power analysis requires three things:\n",
    "\n",
    "Effect size (e.g., Cohen’s d)\n",
    "\n",
    "Sample size (N)\n",
    "\n",
    "Alpha level (e.g., 0.05)\n",
    "\n",
    "Power (usually 0.80)\n",
    "\n",
    "If you don’t know the effect size, you cannot determine:\n",
    "\n",
    "how large your sample should be\n",
    "\n",
    "how likely you are to detect a real effect\n",
    "\n",
    "how strong the difference between groups is\n",
    "\n",
    "Effect size tells you how big the difference is expected to be, which then directly determines how much data you need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c766be-285d-4a07-bdb3-1fbf723771b4",
   "metadata": {},
   "source": [
    "# Estimating required sample size for energy study\n",
    "In the energy sector, researchers are often tasked with evaluating the effectiveness of new technologies or initiatives to enhance energy efficiency or reduce consumption. A study is being designed to compare the impact of two energy-saving measures: \"Smart Thermostats\" and \"LED Lighting\". To ensure the study has sufficient power to detect a meaningful difference in energy savings between these two measures, you'll conduct a power analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b0783b-7318-4c5d-b17a-0afde714cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, \n",
    "import numpy as np\n",
    "from statsmodels.stats.power import TTestIndPower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32acdf7f-bcc7-445c-a835-f6832553ecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a TTestIndPower object\n",
    "power_analysis = TTestIndPower()\n",
    "\n",
    "# Conduct the power analysis to estimate the required sample size for each group (Smart Thermostats and LED Lighting) to achieve a power of 0.9, assuming a moderate effect size (Cohen's d = 0.5) and an alpha of 0.05 with an equal sized groups.\n",
    "required_n = power_analysis.solve_power(\n",
    "    effect_size=0.5, \n",
    "    alpha=0.05, \n",
    "    power=0.9, \n",
    "    ratio=1)\n",
    "\n",
    "print(required_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c2668a-749b-4f48-98e2-9951998a412a",
   "metadata": {},
   "source": [
    "Excellent! By conducting a power analysis, you've determined that approximately 85 participants are required in each group to achieve a power of 0.9, assuming an Cohen's d effect size of 0.5. This information is crucial for planning a sufficiently powered study to compare the energy-saving effectiveness of Smart Thermostats versus LED Lighting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c7b5df-6670-4de4-b777-35daaebbff54",
   "metadata": {},
   "source": [
    "# Your recent learnings\n",
    "When you left 22 hours ago, you worked on Analyzing Experimental Data: Statistical Tests and Power, chapter 3 of the course Experimental Design in Python. Here is what you covered in your last lesson:\n",
    "\n",
    "You learned about power analysis, focusing on understanding effect size and its influence on sample size. Effect size quantifies the magnitude of the difference between groups, beyond just statistical significance. Cohen's d is a common measure, calculated as the difference in means divided by a pooled standard deviation.\n",
    "\n",
    "Key points covered:\n",
    "\n",
    "Effect Size: Quantifies the magnitude of the difference between groups. For example, Cohen's d is calculated as:\n",
    "def cohen_d(group1, group2):\n",
    "\n",
    "    diff_means = np.mean(group1) - np.mean(group2)\n",
    "    \n",
    "    pooled_std = np.sqrt((np.var(group1) + np.var(group2)) / 2)\n",
    "    \n",
    "    return diff_means / pooled_std\n",
    "    \n",
    "Power Analysis: Determines the probability that a test will correctly reject a false null hypothesis (avoiding Type II errors). Power is 1 minus beta (Type II error rate).\n",
    "\n",
    "Sample Size Calculation: Helps determine the necessary sample size to achieve a desired power level. For example, using TTestIndPower to calculate required sample size:\n",
    "\n",
    "from statsmodels.stats.power import TTestIndPower\n",
    "\n",
    "power_analysis = TTestIndPower()\n",
    "\n",
    "required_n = power_analysis.solve_power(effect_size=0.5, alpha=0.05, power=0.9, ratio=1)\n",
    "\n",
    "print(required_n)\n",
    "\n",
    "Balancing Power and Sample Size: Larger sample sizes increase the power of an experiment, enhancing the likelihood of detecting true effects.\n",
    "You also applied these concepts to a video game study, calculating the necessary sample size to achieve 99% power with an assumed effect size.\n",
    "\n",
    "The goal of the next lesson is to learn how to integrate and analyze data from multiple sources to draw meaningful conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f81f51e-d811-41a9-912d-74f667a9986d",
   "metadata": {},
   "source": [
    "# Visualizing loan approval yield\n",
    "In the realm of financial services, understanding the factors that influence loan approval rates is crucial for both lenders and borrowers. A financial institution has conducted a study and collected data on loan applications, detailing the amount requested, the applicant's credit score, employment status, and the ultimate yield of the approval process. This rich dataset offers a window into the nuanced dynamics at play in loan decision-making. You have been asked to dive into the loan_approval_yield dataset to understand how loan amounts and credit scores influence approval yields.\n",
    "\n",
    "The loan_approval_yield DataFrame, seaborn as sns, and matplotlib.pyplot as plt have been loaded for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280c5fd2-307b-478e-93ed-b95e571580d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Seaborn create a side-by-side bar graph, setting the x-axis to 'LoanAmount', the y-axis to 'ApprovalYield', and differentiating the bars with hues for 'CreditScore'.\n",
    "sns.catplot(x=\"LoanAmount\", \n",
    "            y=\"ApprovalYield\", \n",
    "            hue=\"CreditScore\", \n",
    "            kind=\"bar\", \n",
    "            data=loan_approval_yield)\n",
    "plt.title(\"Loan Approval Yield by Amount and Credit Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea43a38c-10ee-426b-a40b-50487e237e9d",
   "metadata": {},
   "source": [
    "What does the analysis of approval yields across different credit scores and loan amounts reveal?\n",
    "\n",
    "answers\n",
    "\n",
    "Poor credit scores tend to have similar approval yields across loan amounts, while Good credit scores have more variability.\n",
    "The data shows that Poor credit scores tend to have similar approval yields across various loan amounts, while Good credit scores exhibit more variability, reflecting different lending criteria based on the loan size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201d0e59-3111-43fc-b291-e3321817747e",
   "metadata": {},
   "source": [
    "# Exploring customer satisfaction\n",
    "\n",
    "Merging datasets is a crucial skill in data analysis, especially when dealing with related data from different sources. You're working on a project for a financial institution to understand the relationship between loan approval rates and customer satisfaction. Two separate studies have been conducted: one focusing on loan approval yield based on various factors, and another on customer satisfaction under different conditions. Your task is to analyze how approval yield correlates with customer satisfaction, considering another variable such as interest rates.\n",
    "\n",
    "The loan_approval_yield and customer_satisfaction DataFrames, pandas as pd, numpy as np, seaborn as sns, and matplotlib.pyplot as plt have been loaded for you.\n",
    "\n",
    "# The presented scenario underscores the necessity of turning complex data into engaging, relatable content. Simplification and effective visualization could significantly enhance comprehension and keep the audience engaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e24856e-6256-4b49-bdf1-902aa0a7316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge loan_approval_yield with customer_satisfaction datasets\n",
    "merged_data = pd.merge(loan_approval_yield, \n",
    "                      customer_satisfaction, \n",
    "                      on='ApplicationID')\n",
    "\n",
    "# Use Seaborn to Create a scatter plot to compare 'SatisfactionQuality' versus 'ApprovalYield', coloring the points by 'InterestRate'.\n",
    "sns.relplot(x=\"ApprovalYield\", \n",
    "            y=\"SatisfactionQuality\", \n",
    "            hue=\"InterestRate\", \n",
    "            kind=\"scatter\", \n",
    "            data=merged_data)\n",
    "plt.title(\"Satisfaction Quality by Approval Yield and Interest Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480afa24-f9fa-4bcd-802b-7608ddc26035",
   "metadata": {},
   "source": [
    "What does the scatterplot of Customer Satisfaction versus Approval Yield, including Interest Rate as a variable, indicate about their relationship in the experimental data?\n",
    "\n",
    "Answer\n",
    "\n",
    "There isn't a strong relationship between Customer Satisfaction and Approval Yield in this experimental data. The resulting scatterplot looks similar to white noise scattered all about even when including Interest Rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8bb2f0-ef7e-46f8-bd80-04816d445f13",
   "metadata": {},
   "source": [
    "# Effectively communicating experimental data\n",
    "You're participating in a research seminar where the latest findings from a neuroscience study are being discussed. The presenter uses a dense slide filled with raw electroencephalogram (EEG) data outputs, complex visualizations, and a small font size, making it difficult for the audience to follow.\n",
    "\n",
    "Given the scenario, what is a key benefit of effectively communicating experimental data?\n",
    "\n",
    "\n",
    "It transforms complex data into engaging, relatable content, enhancing audience understanding. The presented scenario underscores the necessity of turning complex data into engaging, relatable content. Simplification and effective visualization could significantly enhance comprehension and keep the audience engaged.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a261e9-a3be-40b4-8eda-f703fcaf77f5",
   "metadata": {},
   "source": [
    "# Check for heteroscedasticity in shelf life\n",
    "When examining food preservation methods, it's crucial to understand how the variance of one variable, such as shelf life, might change across the range of another variable like nutrient retention. Identifying such patterns, known as heteroscedasticity, can provide insights into the consistency of preservation effects. The food_preservation dataset encapsulates the outcomes of various preservation methods on different food types, specifically highlighting the balance between nutrient retention and resultant shelf life.\n",
    "\n",
    "The food_preservation DataFrame, pandas as pd, numpy as np, seaborn as sns, and matplotlib.pyplot as plt have been loaded for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaef4ec-e159-48a8-86a7-4b46e74718bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for heteroscedasticity with a residual plot(Use an appropriate plot to check for heteroscedasticity between 'NutrientRetention' and 'ShelfLife'.)\n",
    "sns.residplot(x='NutrientRetention', y='ShelfLife', \n",
    "         data=food_preservation, lowess=True)\n",
    "plt.title('Residual Plot of Shelf Life and Nutrient Retention')\n",
    "plt.xlabel('Nutrient Retention (%)')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275e4dbf-a1ef-4022-9a20-29e69a2d1863",
   "metadata": {},
   "source": [
    "The residual plot allows you to visually assess the heteroscedasticity between nutrient retention and shelf life, showing if the spread of residuals changes across nutrient retention levels. You can see some deviation away from the 0 line, so there may be some concerns about heteroscedasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416fb126-e679-4ada-b17b-8c305ed0ccc5",
   "metadata": {},
   "source": [
    "# Exploring and transforming shelf life data\n",
    "Understanding the distribution of different variables in our data is a key aspect of any data work including experimental analysis. The food_preservation dataset captures various food preservation methods and their impact on nutrient retention and shelf life. A crucial aspect of this data involves the shelf life of preserved foods, which can vary significantly across different preservation methods and food types.\n",
    "\n",
    "The food_preservation DataFrame, from scipy.stats import boxcox, pandas as pd, numpy as np, seaborn as sns, and matplotlib.pyplot as plt have been loaded for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5868d19-4432-4edf-86e8-cbadf3f35645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the original ShelfLife distribution\n",
    "sns.displot(food_preservation['ShelfLife'])\n",
    "plt.title('Original Shelf Life Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Create a Box-Cox transformation by Apply a Box-Cox transformation to the 'ShelfLife' column.\n",
    "ShelfLifeTransformed, _ = boxcox(food_preservation['ShelfLife'])\n",
    "\n",
    "# Visualize the transformed ShelfLife distribution\n",
    "plt.clf()\n",
    "sns.displot(ShelfLifeTransformed)\n",
    "plt.title('Transformed Shelf Life Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc95ccf-3399-4fbc-b152-2c9dc0197ffa",
   "metadata": {},
   "source": [
    "Visualizing the original and transformed distributions provides valuable insights into the data's structure. The Box-Cox transformation helps stabilize variance, making the data more suitable for further statistical analysis by helping to make the ShelfLife follow a more normal shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d76d62-c8c1-45ac-a261-45e8020c8095",
   "metadata": {},
   "source": [
    "# Applying non parametric test in experimental analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c8cd02-e08f-4d4a-9ccb-e45fa74b04d0",
   "metadata": {},
   "source": [
    "# Visualizing and testing preservation methods\n",
    "As a food scientist, you're tasked with evaluating the effectiveness of different preservation methods on nutrient retention and how these methods impact shelf life. You have been provided with a dataset, food_preservation, that includes various types of food preserved by methods such as freezing and canning. Each entry in the dataset captures the nutrient retention and calculated shelf life for these foods, providing a unique opportunity to analyze the impacts of preservation techniques on food quality.\n",
    "\n",
    "The following imports have been loaded for you in addition to food_preservation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37223455-a212-4eb5-bf41-9ae289c4b72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mannwhitneyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cec856a-39d2-4e14-9b17-984a211362b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only Freezing and Canning rows.\n",
    "condensed_food_data = food_preservation[food_preservation['PreservationMethod'].isin(['Freezing', 'Canning'])]\n",
    "\n",
    "# Create a violin plot for nutrient retention by preservation method(Create a violin plot to visualize the distribution of nutrient retention for different preservation methods.)\n",
    "sns.violinplot(data=condensed_food_data, \n",
    "     x=\"PreservationMethod\", \n",
    "     y=\"NutrientRetention\")\n",
    "plt.show()\n",
    "\n",
    "# Separate nutrient retention for Freezing and Canning methods\n",
    "freezing = food_preservation[food_preservation['PreservationMethod'] == 'Freezing']['NutrientRetention']\n",
    "canning = food_preservation[food_preservation['PreservationMethod'] == 'Canning']['NutrientRetention']\n",
    "\n",
    "# Perform Mann Whitney U test\n",
    "u_stat, p_val = mannwhitneyu(freezing,canning)\n",
    "\n",
    "# Print the p-value\n",
    "print(\"Mann Whitney U test p-value:\", p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8e24b2-bdc1-4ace-9cbd-7fd56f7f62fd",
   "metadata": {},
   "source": [
    "The violin plot shows that the distribution and median values are similar across Freezing and Canning. The large p-value leads us to suspect that a statistical difference does not exist in the medians of nutrient retention for freezing versus canning preservation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd556cfa-b32c-416c-89b2-8bacab151cd0",
   "metadata": {},
   "source": [
    "# Further analyzing food preservation techniques\n",
    "In your role as a food scientist, you're exploring into the comparative effects of various food preservation methods on nutrient retention, utilizing a food_preservation dataset that includes measurements from freezing, canning, and drying methods. This dataset has been crafted to incorporate variations in shelf life that depend on the nutrient retention values, reflecting real-world scenarios where preservation efficacy varies significantly. Your analysis will involve visually exploring these differences using advanced plotting techniques and nonparametric tests.\n",
    "\n",
    "The following imports have been loaded for you in addition to food_preservation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d80ecf8-33ca-48d2-83a6-3087c89525c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedf0149-664b-4240-8aba-ea2f4198d2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boxen plot to explore the distribution of nutrient retention across the three different preservation methods.\n",
    "sns.boxenplot(data=food_preservation, \n",
    "     x=\"PreservationMethod\", \n",
    "     y=\"NutrientRetention\")\n",
    "plt.show()\n",
    "\n",
    "# Separate nutrient retention for each preservation method\n",
    "freezing = food_preservation[food_preservation['PreservationMethod'] == 'Freezing']['NutrientRetention']\n",
    "canning = food_preservation[food_preservation['PreservationMethod'] == 'Canning']['NutrientRetention']\n",
    "drying = food_preservation[food_preservation['PreservationMethod'] == 'Drying']['NutrientRetention']\n",
    "\n",
    "# Perform Kruskal-Wallis test to compare nutrient retention across all preservation methods.\n",
    "k_stat, k_pval = kruskal(freezing,canning,drying)\n",
    "print(\"Kruskal-Wallis test p-value:\", k_pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dee86b-923b-493a-9215-c961a1bc5a19",
   "metadata": {},
   "source": [
    "By effectively visualizing and statistically analyzing the nutrient retention across different preservation methods, you've gained insights into how these methods impact food quality. The boxen plot provided a deeper understanding of the data's distribution, and the Kruskal-Wallis test helped you assess the statistical differences between groups. The large p-value leads us to fail to conclude that a difference in the median values across the three groups of preservation methods exists for nutrient retention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca61454-05e1-45b2-934a-e0614307434f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eb778b-47f7-41d0-8c8a-5591c421ad9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53393823-ec9c-4989-b1fc-3a33ea12fd79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57568a6-7244-48be-96ee-0f7e3e33450c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f04f9bf-f943-495a-ac1d-dcb21fe83705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c63e33-2a02-4ffb-81d3-f3414044f41a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005d8a0e-6e46-4749-9b87-b85debee27a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
